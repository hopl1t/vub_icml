#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{IB}

\bibliographystyle{icml2023}
\end_preamble
\options letterpaper
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Tighter Bounds on the Information Bottleneck with Application to Deep Learning
\end_layout

\begin_layout Author
Anonymous submission
\end_layout

\begin_layout Abstract
Deep Neural Nets (DNNs) learn latent representations induced by their downstream
 task, objective function and other parameters.
 The quality of their data modeling affects the DNN's ability to generalize
 and the coherence of the emerging latent space.
 The Information Bottleneck, IB, offers an optimal information theoretic
 framework for data modeling, however, it is intractable in most settings.
 In recent years attempts were made to combine deep learning with IB both
 for optimization and for the interpretation of DNNs.
 VAE inspired variational approximations became a popular method to approximate
 bounds on the required mutual information computations.
 This work introduces a new tractable variational upper bound for the IB
 functional which is tighter than previous bounds.
 When used as an objective function it enhances the performance of previous
 IB-inspired DNNs in terms of test accuracy while providing similar or superior
 robustness to adversarial attacks.
 These improvements in performance, induced by a tighter bound, strengthen
 the cause for the IB and it's variational approximations as a framework
 for better representation learning.
 Furthermore, we present practitioners with a method to substantially increase
 adversarial robustness of any classifier DNN while suffering only a slight
 decrease in performance over unseen data.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In recent years deep neural nets have gained increasing popularity in different
 learning tasks.
 Their ability to approximate complicated functions has revolutionized many
 computational fields.
 Despite the great achievements, it is still postulated that the current
 networks are prone to overfit the training data 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Ying2019}
\end_layout

\end_inset

, may be considerably uncalibrated 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Guo2017}
\end_layout

\end_inset

 and are susceptible to adversarial attacks 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Goodfellow2015}
\end_layout

\end_inset

.
 A question emerges regarding the extraction of an optimal representation
 for all data points from our restricted set of training examples.
 Classic information theory provides tools to optimize compression and transmiss
ion of data, but it does not provide methods to gauge the relevance of the
 signal to downstream tasks.
 Methods such as rate distortion 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Blahut1972}
\end_layout

\end_inset

 regard all information as equal, not taking into account which information
 is more relevant without constructing complex distortion functions.
 To resolve these limitations, the Information Bottleneck, IB 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

, defines an information theoretic limit for the rate distortion ratio of
 any learning task given a Lagrangian hyper parameter 
\begin_inset Formula $\beta$
\end_inset

 controlling the tradeoff between the rate and the distortion.
 Optimizing a learning task using the IB objective results in an encoding
 with optimal rate distortion ratio for a downstream task, over a chosen
 Lagrangian 
\begin_inset Formula $\beta$
\end_inset

 and for a mutual information distortion function.
 Computing the IB requires mutual information calculations which are tractable
 in discrete settings and for some specific continuous distributions.
 Adopting the IB framework for DNNs requires computing mutual information
 for unknown distributions and has no analytic solution.
 However, recent work approximated tractable upper bounds for the IB functional
 in DNN settings using variational approximations.
 Variational auto encoders 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

 use stochastic DNNs to approximate intractable and unknown distributions.
 Assuming a latent variable model the marginal 
\begin_inset Formula $p(x)=\int p(x|z)p(z)dz$
\end_inset

 (Where 
\begin_inset Formula $x\in X$
\end_inset

 is an observed variable and 
\begin_inset Formula $z\in Z$
\end_inset

 is unobserved) can be approximated using a chosen variational distribution
 optimized to fit the training data.
 This optimization is possible using Stochastic Gradient Descent (SGD) and
 the 'reparameterization trick' as elaborated in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:variational_approximations"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Similarly to VAEs 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

 proposed using stochastic DNNs as variational approximations for a latent
 model thus making possible the computations of upper bounds for mutual
 information between the DNN's input, output and hidden layers.
 A proposed optimization method called VIB - 'Deep Variational Information
 Bottleneck' derives an upper bound for the IB objective and minimizes it's
 approximation using Monte Carlo sampling over training data.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

 found that replacing a DNN's deterministic classifier layer with a stochastic
 layer optimized with the VIB objective results in a slight decrease in
 test set accuracy but a significant increase in robustness to adversarial
 attacks in complicated classification tasks.
 Similar behavior is demonstrated in the current work as shown in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{sec:Experiments}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
The case for the IB functional, and it's variational approximations, as
 a theoretical limit for optimal representation relies on three assumptions:
 (1) It suffices to optimize the mutual information metric to optimize a
 model's performance; (2) Forgetting more information about the input while
 keeping the same information about the output induces better generalization
 over unseen data; (3) Mutual information between the input, output and
 latent representation can be either computed or approximated to a desired
 level of accuracy.
\end_layout

\begin_layout Standard
In this study, we adopt the same information theoretic and variational approach
 used previously.
 The work begins by deriving an upper bound for the IB functional.
 We then employ a tractable variational approximation for this bound, named
 VUB - 'Variational Upper Bound' and show that it is a tighter bound on
 IB than VIB.
 We proceed to show empirical evidence that VUB substantially increases
 test accuracy over VIB while providing similar or superior robustness to
 adversarial attacks across several challenging tasks and modalities.
 Finally, we discuss these effects in the context of previous work on IB
 and other DNN regularization techniques.
 The conclusion drawn is that while more mutual information between encoding
 and output does not necessarily improve classification, and more compressed
 encoding does not always enhance regularization 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Amjad2020}
\end_layout

\end_inset

, the application of IB approximations as objectives to DNNs empirically
 improves regularization, suggesting better data modeling.
 This notion contributes for the adaptation of IB, and it's variational
 approximations, as an objective to learning tasks and as a theoretic framework
 to gauge and explain regularization.
\end_layout

\begin_layout Standard
In addition, we demonstrate that VUB can be easily adapted to any classifier
 DNN, including transformer based NLP classifiers, to substantially increase
 robustness to adversarial attacks with only a small decrease in test accuracy.
\end_layout

\begin_layout Subsection
Preliminaries
\end_layout

\begin_layout Standard
The following literature review and derivations refer to information theory
 and variational approximations.
 A preliminary mutual ground and notation follows:
\end_layout

\begin_layout Standard
We denote random variables with upper cased letters 
\begin_inset Formula $X,Y$
\end_inset

 and their realizations in lower case 
\begin_inset Formula $x,y$
\end_inset

.
 Denote discrete Probability Mass Functions (PMFs) with an upper case 
\begin_inset Formula $P(X)$
\end_inset

 and continuous Probability Density Functions (PDFs) with a lower case 
\begin_inset Formula $p(x)$
\end_inset

.
 Hat notation denotes empirical measurements.
 Let 
\begin_inset Formula $X,Y$
\end_inset

 be two observed random variables (RVs) with unknown distributions 
\begin_inset Formula $p^{*}(x),p^{*}(y)$
\end_inset

 that we aim to model.
 Assume 
\begin_inset Formula $X,Y$
\end_inset

 are governed by some unknown underlying process with a joint probability
 distribution 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

.
 We can attempt to approximate these distributions using a model 
\begin_inset Formula $p_{\theta}$
\end_inset

 with parameters 
\begin_inset Formula $\theta$
\end_inset

 such that for generative tasks 
\begin_inset Formula $p_{\theta}(x)\approx p^{*}(x)$
\end_inset

 and for discriminative tasks 
\begin_inset Formula $p_{\theta}(y|x)\approx p^{*}(y|x)$
\end_inset

, using a dataset 
\begin_inset Formula $\mathcal{S}=\left\{ (x_{1},y_{1}),...,(x_{N},y_{N})\right\} $
\end_inset

 to fit our model.
 One can also assume the existence of an additional unobserved RV 
\begin_inset Formula $Z\sim p^{*}(z)$
\end_inset

 that influences or generates the observed RVs 
\begin_inset Formula $X,Y$
\end_inset

.
 Since 
\begin_inset Formula $Z$
\end_inset

 is unobserved it is absent from the dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

 and so cannot be modeled directly.
 Denote 
\begin_inset Formula $\int p_{\theta}(x|z)p_{\theta}(z)dz$
\end_inset

 the marginal, 
\begin_inset Formula $p_{\theta}(z)$
\end_inset

 the prior as it is not conditioned over any other RV, and 
\begin_inset Formula $p_{\theta}(z|x)$
\end_inset

 the posterior following Bayes' rule.
 Latent models are frequently used for creating variational bounds on mutual
 information as they make good approximations for 
\begin_inset Formula $p^{*}(x)$
\end_inset

 due to their high expressivity 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2019}
\end_layout

\end_inset

, as is demonstrated in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:variational_approximations"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Deep neural nets, or DNNs, are a powerful tool to approximate complicated
 functions such as modeling 
\begin_inset Formula $p_{\theta}(y|x)$
\end_inset

 or 
\begin_inset Formula $p_{\theta}(x)$
\end_inset

.
 When choosing a DNN to approximate a latent model we encounter a problem
 as the marginal integral 
\begin_inset Formula $p_{\theta}(x)=\int p_{\theta}(x,z)dz$
\end_inset

 doesn't have an analytic solution, and is not optimizable over gradient
 descent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2019}
\end_layout

\end_inset

.
 This intractability can be overcome by using a tractable parametric variational
 encoder 
\begin_inset Formula $q_{\phi}(z|x)$
\end_inset

 to approximate the posterior 
\begin_inset Formula $p_{\theta}(z|x)$
\end_inset

 such that 
\begin_inset Formula $q_{\phi}(z|x)\approx p_{\theta}(z|x)$
\end_inset

, and estimate 
\begin_inset Formula $p_{\theta}(x,z)$
\end_inset

 or 
\begin_inset Formula $p_{\theta}(x,z|y)$
\end_inset

 using Monte Carlo sampling from the dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

 during optimization.
 In this work information theoretic functions share the same notation for
 discrete and continuous settings and are denoted as follows:
\end_layout

\begin_layout Standard
\align left
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features tabularvalignment="middle">
<column alignment="left" valignment="middle" width="1.8cm">
<column alignment="left" valignment="middle" width="5cm">
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $H_{p}(X)=-\int p(x)log\left(p(x)\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cross
\end_layout

\begin_layout Plain Layout
entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $CH(P,Q)=-\int p(x)log\left(q(x)\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
KL
\end_layout

\begin_layout Plain Layout
divergence
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $D_{KL}\left(P\Big|\Big|Q\right)=\int p(x)log\left(\frac{p(x)}{q(x)}\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mutual information
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $I(X;Y)=\int\int p(x,y)log\left(\frac{p(x,y)}{p(x)p(y)}\right)dxdy$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Section
Related work
\end_layout

\begin_layout Subsection
IB and it's analytic solutions
\end_layout

\begin_layout Standard
Classic information theory offers rate-distortion 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Blahut1972}
\end_layout

\end_inset

 to mitigate signal loss during compression.
 Rate being the signal's compression measured by mutual information between
 input and output signals, and distortion a chosen task-specific function.
 The Information Bottleneck method 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

 extends rate-distortion by optimizing a signal's compression to a given
 downstream task by learning an encoding that holds just enough information
 about the source signal to enable a distortion above a chosen positive
 threshold 
\begin_inset Formula $D$
\end_inset

.
 Denote 
\begin_inset Formula $X$
\end_inset

 the source signal, 
\begin_inset Formula $Z$
\end_inset

 it's encoding and 
\begin_inset Formula $Y$
\end_inset

 the target signal for some specific downstream task, assume a latent variable
 model with the Markov chain 
\begin_inset Formula $Z\leftarrow X\leftarrow Y$
\end_inset

 and define the distortion function as mutual information between encoding
 and downstream task.
 Hence, we seek an optimal encoding 
\begin_inset Formula $Z:\underset{P(Z|X)}{min}I(X;Z)$
\end_inset

 subject to 
\begin_inset Formula $I(Z;Y)\ge D$
\end_inset

.
 This constrained problem can be implicitly optimized by minimizing the
 functional 
\begin_inset Formula $L_{P(Z|X)}=I(Z;X)-\beta I(Z;Y)$
\end_inset

 over the Lagrangian 
\begin_inset Formula $\beta$
\end_inset

, the first term being rate and the second distortion.
 The optimal solution is a function of 
\begin_inset Formula $\beta$
\end_inset

 and was named 'the information curve' as shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../aaai/media/info_plane.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The information plane and curve - rate distortion ratio over 
\begin_inset Formula $\beta$
\end_inset

.
 Adapted from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Slonim2002}
\end_layout

\end_inset

.
 At 
\begin_inset Formula $\beta=0$
\end_inset

, representation is compressed but uninformative (maximal compression),
 at 
\begin_inset Formula $\beta\rightarrow\infty$
\end_inset

 it's informative but potentially overfitted (maximal information).
\begin_inset CommandInset label
LatexCommand label
name "info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The IB functional is only tractable when mutual information can be computed
 and was demonstrated by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Tishby1999}
\end_layout

\end_inset

 for soft clustering tasks over a discrete and known distribution 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Chechik2003}
\end_layout

\end_inset

 extended IB for gaussian distributions and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Painsky2017}
\end_layout

\end_inset

 offered a limited linear approximation of IB for any distribution.
\end_layout

\begin_layout Subsection
IB and deep learning
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Tishby2015}
\end_layout

\end_inset

 proposed an IB interpretation of DNNs regarding them as Markov cascades
 of intermediate representations between hidden layers.
 Neural architecture and dataset cardinality are theoretically sufficient
 to compute the optimal rate distortion of a DNN on the information curve.
 This suggests a model 
\emph on
complexity gap
\emph default
 between the achieved and optimal IB compression rate for that setting,
 and it is hypothesized that bridging this gap will result in optimal generaliza
tion.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

 visualized and analyzed the information plane behavior of DNNs over a toy
 problem with a known joint distribution 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

.
 Mutual information of the different layers was estimated and used to analyze
 the training process.
 SGD exhibited two separate and sequential behaviors during training: A
 short empirical error minimization phase characterized by larger gradient
 norms and a rapid decrease in distortion, followed by a long compression
 phase with smaller gradient norms and an increase in rate until convergence
 to an optimal IB limit as demonstrated in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{shwartz_ziv_info_plane}
\end_layout

\end_inset

.
\series bold

\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../../aaai/media/black_box.png
	scale 37

\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
\begin_inset Caption Standard

\begin_layout Plain Layout
Information plane scatters of different DNN layers (colors) in 50 randomized
 networks.
 From 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.
 Left are initial weights, Right are at 400 epochs.
 
\begin_inset Formula $I(T;Y),I(X;T)$
\end_inset

 are analogous to 
\begin_inset Formula $I(Z;Y),I(X;Z)$
\end_inset

 in the current study.
 We believe to be the first to demonstrate similar information plane behavior
 on real-world problems as shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{estimated_info_plane}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "shwartz_ziv_info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Amjad2020}
\end_layout

\end_inset

 pointed out three flaws in the usage of the IB functional as an objective
 for deterministic DNN classifiers: (1) When data 
\begin_inset Formula $X$
\end_inset

 is absolutely continuous the mutual information term 
\begin_inset Formula $I(X;Z)$
\end_inset

 is infinite; (2) When data 
\begin_inset Formula $X$
\end_inset

 is discrete the IB functional is a piecewise constant function of the parameter
s, making it's SGD optimization difficult or impossible; (3) Equivalent
 representations might yield the same IB loss while one achieved much better
 classification rate than the other.
 These discrepancies were attributed to to mutual information's invariance
 to invertible transformations and the absence of the decision function
 in the objective.
\end_layout

\begin_layout Subsection
Variational approximations to the IB objective
\begin_inset CommandInset label
LatexCommand label
name "subsec:variational_approximations"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Kingma2014}
\end_layout

\end_inset

 introduced the variational auto encoder - a stochastic generative DNN.
 An unobserved RV 
\begin_inset Formula $Z$
\end_inset

 is assumed to generate evidence 
\begin_inset Formula $X,Y$
\end_inset

 and the true probability 
\begin_inset Formula $p^{*}(x)$
\end_inset

 can be modeled using a parametric model over the marginal 
\begin_inset Formula $p_{\theta}(x)=\int p_{\theta}(x|z)p_{\theta}(z)dz$
\end_inset

.
 However, since the marginal is intractable a variational approximation
 
\begin_inset Formula $q_{\phi}(z|x)\approx p_{\theta}(z|x)$
\end_inset

 is proposed instead.
 The log probability 
\begin_inset Formula $log\left(p_{\theta}(x)\right)$
\end_inset

 is then developed in to the tractable VAE loss comprised of the Evidence
 Lower Bound (ELBO) and KL regularization terms: 
\begin_inset Formula $\underset{\text{\emph{\text{\emph{ELBO}}}}}{\mathcal{L}}=\mathbb{E}_{q_{\phi}(z|x)}\left[log\left(p_{\theta}(x|z)\right)\right]-D_{KL}\left(q_{\phi}(z|x)\Big|\Big|p_{\theta}(z)\right)$
\end_inset

.
 
\begin_inset Formula $q_{\phi}(z|x)$
\end_inset

 is modeled using a stochastic neural encoder having it's final activation
 used as parameters for the assumed variational distribution (typically
 a spherical gaussian with parameters 
\begin_inset Formula $\mu,\Sigma$
\end_inset

).
 Each forward pass emulates a stochastic realization 
\begin_inset Formula $z\in Z$
\end_inset

 from these parameters by using the 'reparameterization trick': 
\begin_inset Formula $z=\mu+\epsilon\cdot\Sigma$
\end_inset

 for some unparameterized scalar 
\begin_inset Formula $\epsilon\sim N(0,1)$
\end_inset

 s.t.
 a backwards pass is possible.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Higgins2017}
\end_layout

\end_inset

 later proposed the 
\begin_inset Formula $\beta$
\end_inset

-autoencoder introducing a hyper parameter 
\begin_inset Formula $\beta$
\end_inset

 over the KL term to control the regularization-reconstruction tradeoff.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

 introduced the Variational Information Bottleneck (VIB) as a variational
 approximation for an upper bound to the IB objective for classifier DNN
 optimization: Upper bounds for 
\begin_inset Formula $I(Z,Y),I(X,Z)$
\end_inset

 are derived from the identity 
\begin_inset Formula $D_{KL}\left(\text{True probability}\Big|\Big|\text{Variational approximation}\right)\ge0$
\end_inset

 and used to form an upper bound for the IB functional.
 This upper bound is approximated using variational approximations for 
\begin_inset Formula $p^{*}(y|z),p^{*}(z)$
\end_inset

 similarly to VAEs.
 This approximation to an upper bound over the IB objective is empirically
 estimated as Cross entropy and a beta scaled KL regularization term as
 in 
\begin_inset Formula $\beta$
\end_inset

-autoencoders and is optimized over the training data 
\begin_inset Formula $\mathcal{S}$
\end_inset

 using Monte Carlo sampling and the reparameterization trick.
 VIB was evaluated over MNIST and ImageNet and, while causing a slight reduction
 in test-accuracy, it generated substantial improvements in robustness to
 adversarial attacks.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 found that as the ELBO loss in VAEs depends solely on image reconstruction
 it does not necessarily induce a better quality modeling of the marginal
 
\begin_inset Formula $p_{\theta}(z)$
\end_inset

, hence not necessarily a better representation learned.
 This gap is attributed to powerful decoders being overfitted.
 It is suggested to keep 
\begin_inset Formula $\beta$
\end_inset

 values under 
\begin_inset Formula $1$
\end_inset

 and monitor the rate-distortion tradeoff as well as cross-entropy loss.
\end_layout

\begin_layout Standard
Additional noteworthy contributions to this field have been made in recent
 years by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Achille2018,Wieczorek2019,Fischer2020}
\end_layout

\end_inset

 and others.
 However, a detailed review of these works is beyond the scope of this paper.
\end_layout

\begin_layout Subsection
Non IB information theoretic regularization
\end_layout

\begin_layout Standard
Label smoothing 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Szegedy2016}
\end_layout

\end_inset

 and entropy regularization 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Pereyra2017}
\end_layout

\end_inset

 both regularize classifier DNNs by increasing the entropy of their output.
 This is done either directly with an entropy term in the loss function,
 
\begin_inset Formula $\beta\cdot H\left(p_{\theta}(y|x)\right),$
\end_inset

 or by smoothing the training data labels.
 Applying both methods was demonstrated to improve test accuracy and model
 calibration.
 In the current work a similar conditional entropy term emerges from the
 derivation of the new upper bound for the IB objective as shown in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:vub"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Section
From VIB to VUB
\begin_inset CommandInset label
LatexCommand label
name "sec:vub"

\end_inset


\end_layout

\begin_layout Standard
The VIB loss 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017}
\end_layout

\end_inset

 is approximated as a CH term and a KL regularization term, similarly to
 VAE loss.
 The KL term is derived from a bound on the IB rate term 
\begin_inset Formula $I(X;Z)$
\end_inset

 and the CH term from a bound on the IB distortion term 
\begin_inset Formula $I(Z;Y)$
\end_inset

.
 When deriving the distortion term the entropy term 
\begin_inset Formula $H(Y)$
\end_inset

 is ignored as it is constant and does not effect optimization, while the
 conditional entropy term 
\begin_inset Formula $H(Y|Z)$
\end_inset

 is derived into a cross entropy term.
 Since 
\begin_inset Formula $H(Y)\ge H(Y|Z)$
\end_inset

 we can approximate the ignored 
\begin_inset Formula $H(Y)$
\end_inset

 term and acheive a tighter bound on the IB functional while improving data
 modeling via decoder regularization.
\end_layout

\begin_layout Subsection
IB upper bound
\end_layout

\begin_layout Standard
We begin by establishing a new upper bound for the IB functional.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $I(Z;X)$
\end_inset

 we follow the same derivation as per VIB:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
I(Z;X)= & \int\int p^{*}(x,z)log\left(p^{*}(z|x)\right)dxdz\\
- & \int p^{*}(z)log\left(p^{*}(z)\right)dz\tag{{1.0}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
For any probability distribution 
\begin_inset Formula $r$
\end_inset

 we have that: 
\begin_inset Formula $D_{KL}\left(p^{*}(z),r(z)\right)\ge0\Longrightarrow\int p^{*}(z)log\left(p^{*}(z)\right)dz\ge\int p^{*}(z)log\left(r(z)\right)dz$
\end_inset

 and so:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
I(Z;X)\overset{\text{(1.0)}}{\le}\int\int p^{*}(x)p^{*}(z|x)log\left(\frac{p^{*}(z|x)}{r(z)}\right)dxdz\tag{{1.1}}
\]

\end_inset


\end_layout

\begin_layout Standard
For 
\begin_inset Formula $I(Z;Y)$
\end_inset

 we begin with the same derivation as per VIB:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
 & D_{KL}\left(p^{*}(y|z),c(y|z)\right)\ge0\Longrightarrow\\
 & \int p^{*}(y|z)log\left(p^{*}(y|z)\right)dy\ge\int p^{*}(y|z)log\left(c(y|z)\right)dy\tag{{1.2}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and hence for 
\begin_inset Formula $I(Z;Y)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
I(Z;Y)= & \int\int p^{*}(y,z)log\left(\frac{p^{*}(y,z)}{p^{*}(y)p^{*}(z)}\right)dydz\\
\overset{\text{\text{(1.2)}}}{\ge} & \int\int p^{*}(y|z)p^{*}(z)log\left(\frac{c(y|z)}{p^{*}(y)}\right)dydz\\
\overset{\text{}}{=} & \int\int p^{*}(y,z)log\left(c(y|z)\right)dydz+H_{p^{*}}(Y)\tag{{1.3}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We proceed without discarding the entropy of 
\begin_inset Formula $Y$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\ge & \int\int p^{*}(y,z)log\left(c(y|z)\right)dydz\\
+ & min\left\{ H_{p^{*}}(Y),H_{c}(Y|Z)\right\} \\
= & \int\int p^{*}(y,z)log\left(c(y|z)\right)dydz\\
+ & min\left\{ H_{p^{*}}(Y),-\int\int c(y,z)log\left(c(y|z)\right)dydz\right\} \tag{{2.0}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We further develop this term using the markov chain 
\begin_inset Formula $Z\leftarrow X\leftarrow Y$
\end_inset

 and total probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
 & I(Z;Y)\ge\\
 & \int\int\int p^{*}(x)p^{*}(y|x)p^{*}(z|x)log\left(c(y|z)\right)dxdydz\\
- & min\left\{ H_{p^{*}}(Y),-\int\int c(y,z)log\left(c(y|z)\right)dydz\right\} \tag{{2.1}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We define 
\begin_inset Formula $L_{UB}$
\end_inset

 - an upper bound for IB using the bound on the distortion term derived
 in (2.1) and the bound on the rate term derived in (1.1):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
 & L_{UB}\equiv\\
 & \int\int p^{*}(x)p^{*}(z|x)log\left(\frac{p^{*}(z|x)}{r(z)}\right)dxdz\\
- & \int\int\int p^{*}(x)p^{*}(y|x)p^{*}(z|x)log\left(c(y|z)\right)dxdydz\\
+ & min\left\{ H_{p^{*}}(Y),-\int\int c(y,z)log\left(c(y|z)\right)dydz\right\} \tag{{2.2}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
It is easy to verify that the derivation holds for all 
\begin_inset Formula $\beta\ge0$
\end_inset

 such that 
\begin_inset Formula $L_{IB}=\beta\cdot I\left(Z;X\right)-I\left(Z;Y\right)$
\end_inset

.
\end_layout

\begin_layout Subsection
Variational approximation
\end_layout

\begin_layout Standard
We follow the same variational approach as in VIB.
 We define 
\begin_inset Formula $L_{VUB}$
\end_inset

 as a new tractable upper bound for IB.
 Let 
\begin_inset Formula $p^{*}(x,y,z)$
\end_inset

 be the unknown joint distribution, 
\begin_inset Formula $e(z|x)$
\end_inset

 a variational encoder approximating 
\begin_inset Formula $p^{*}(z|x)$
\end_inset

, 
\begin_inset Formula $c(y|z)$
\end_inset

 a variational classifier approximating 
\begin_inset Formula $p^{*}(y|z)$
\end_inset

.
 Since 
\begin_inset Formula $p^{*}(z)$
\end_inset

 is intractable we use a variational approximation 
\begin_inset Formula $r(z)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
 & L_{VUB}\equiv\tag{{2.3}}\\
 & \beta\int\int p^{*}(x)e_{\phi}(z|x)log\left(\frac{e_{\phi}(z|x)}{r(z)}\right)dxdz\\
- & \int\int\int p^{*}(x)p^{*}(y|x)e_{\phi}(z|x)log\left(c_{\lambda}(y|z)\right)dxdydz\\
- & min\Bigg\{ H_{p^{*}}(Y),\\
 & -\int\int\int p^{*}(x)e_{\phi}(z|x)c_{\lambda}(y|z)log\left(c_{\lambda}(y|z)\right)dxdydz\Bigg\}\\
\ge & L_{IB}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Empirical estimation
\end_layout

\begin_layout Standard
We proceed to model VUB using DNNs and optimize it using Monte Carlo sampling
 over the training data 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
 Let 
\begin_inset Formula $e_{\phi}$
\end_inset

 be a stochastic DNN encoder with parameters 
\begin_inset Formula $\phi$
\end_inset

 applying the reparameterization trick 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

 such that 
\begin_inset Formula $e_{\phi}(x)\sim N(\mu,\Sigma)$
\end_inset

.

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 Let 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $C_{\lambda}$
\end_inset

 be a discrete classifier DNN parameterized by 
\begin_inset Formula $\lambda$
\end_inset

 such that 
\begin_inset Formula $C_{\lambda}(\hat{z})\sim Categorical$
\end_inset

.
 We reintroduce 
\begin_inset Formula $1\le\beta\le0$
\end_inset

 to allow tuning of the decoder entropy term.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\hat{L}_{VUB} & \equiv\\
 & \frac{1}{N}\sum_{n=1}^{N}\Bigg[\beta\cdot D_{KL}\left(e_{\phi}(x_{n})\Bigg|\Bigg|r(z)\right)\\
- & P^{*}(y_{n})\cdot log\left(C_{\lambda}\left(e_{\phi}(x_{n})\right)\right)\\
- & min\left\{ H(\hat{Y}),H\left(C_{\lambda}\left(e_{\phi}(x_{n})\right)\right)\right\} \Bigg]\tag{{2.4}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
As in VIB and VAE 
\begin_inset Formula $e_{\phi}(x)$
\end_inset

 is a computed as spherical gaussian by using the first half of the encoder's
 output entries as 
\begin_inset Formula $\mu$
\end_inset

 and the second as the diagonal 
\begin_inset Formula $\Sigma$
\end_inset

.
\end_layout

\begin_layout Subsection
Interpretation
\end_layout

\begin_layout Standard
VUB is in fact VIB regulated by a conditional entropy term 
\begin_inset Formula $-H(Y|Z)$
\end_inset

 similarly to the confidence penalty suggested by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Pereyra2017}
\end_layout

\end_inset

.
 Hence, VUB adds regularization over the classifier preventing it to overfit
 the embeddings.
 This is a possible remedy to the discrepancies in the ELBO loss observed
 by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

.
 In terms of tightness we have that VUB is a tighter theoretical bound on
 IB than VIB for any 
\begin_inset Formula $Y$
\end_inset

 s.t.
 
\begin_inset Formula $H(Y)>0$
\end_inset

, and a tighter empirical bound for all 
\begin_inset Formula $Y$
\end_inset

 in discrete classification tasks.
\end_layout

\begin_layout Section
Experiments
\begin_inset CommandInset label
LatexCommand label
name "sec:Experiments"

\end_inset


\end_layout

\begin_layout Standard
We follow the experimental setup used by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

.
 In addition we tracked IB metrics to provide quantitive assessment of the
 learning process.
 Image classification models were trained on the first 500,000 samples of
 the ImageNet 2012 dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{deng2009imagenet}
\end_layout

\end_inset

 and text classification over
\series bold
 
\series default
the IMDB
\series bold
 
\series default
sentiment analysis dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{maas2011imdb}
\end_layout

\end_inset

.
 For each dataset, a competitive pre-trained model (Vanilla model) was evaluated
 and then used to encode embeddings.
 These embeddings were then used as a dataset for a new stochastic classifier
 net with either a VIB or a VUB loss function.
 Stochastic classifiers consisted of two ReLU activated linear layers of
 the same dimensions as the pre-trained model's logits (2048 for image and
 768 for text classification), followed by reparameterization and a final
 softmax activated FC layer.
 Learning rate was 
\begin_inset Formula $10^{-4}$
\end_inset

 with an exponential decay with a factor of 0.97 every two epochs.
 Batch sizes were 32 for ImageNet and 16 for IMDB.
 We used a single forward pass per sample for inference.
 Each model was trained and evaluated 5 times per 
\begin_inset Formula $\beta$
\end_inset

 value with consistent performance and we display the mean and standard
 deviation per 
\begin_inset Formula $\beta$
\end_inset

 setting.
 We tried 
\begin_inset Formula $\beta=10^{-i}$
\end_inset

 for 
\begin_inset Formula $i\in\{0,1,2,3\}$
\end_inset

 since previous studies indicated this is the best range for VIB 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017,Alemi2018}
\end_layout

\end_inset

.
 Each model was evaluated using test set accuracy and robustness to two
 different adversarial attacks over the test set.
 For image classification, we employed untargeted Fast Gradient Sign (FGS)
 attacks 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Goodfellow2015}
\end_layout

\end_inset

 as well as targeted 
\begin_inset Formula $L_{2}$
\end_inset

 optimization attacks 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Carlini2017}, 
\backslash
cite{Kaiwen2018}
\end_layout

\end_inset

.
 For text classification, we used the Deep Word Bug method 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{gao2018deepwordbug}, 
\backslash
cite{Morris2020}
\end_layout

\end_inset

 as well as the PWWS attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{ren2019pwws}
\end_layout

\end_inset

.
 Code to reconstruct the experiments is provided in the code & data appendix.
 All models were trained using an Nvidia RTX3080 GPU.
\end_layout

\begin_layout Subsection
Image classification
\end_layout

\begin_layout Standard
A pre-trained inceptionV3 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Szegedy2016}
\end_layout

\end_inset

 base model was used and achieved a 77.21% accuracy on the image-net 2012
 validation set (Test set for image-net is unavailable).
 Note that inceptionV3 yields a slightly worse single shot accuracy than
 inceptionV2 (80.4%) when run in a single model and single crop setting,
 however we've used InceptionV3 over V2 for simplicity.
 Each model was trained for 100 epochs.
\end_layout

\begin_layout Subsubsection
Evaluation and analysis
\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="5">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Val 
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\underset{\epsilon=0.1}{\text{FGS}}\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\underset{\epsilon=0.5}{\text{FGS}}\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
CW
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vanilla model
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
77.2%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
68.9%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
67.7%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
788
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VIB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
73.7%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
59.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
63.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3917
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm291$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
72.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
53.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3318
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm293$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
72.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.01\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
58.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3318
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm293$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VUB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.3%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.06\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
66.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3031
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm351$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.03\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
57.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.06\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
64.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1634
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm209$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
74.7%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.04\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
57.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
64.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3628
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.6$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Image-net evaluation scores for vanilla, VIB and VUB models, average over
 5 runs with standard deviation.
 First column is performance on the image-net validation set.
 Second and third columns are the % of successful FGS attacks at 
\begin_inset Formula $\epsilon=0.1,0.5$
\end_inset

 (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

).
 Fourth column is the average 
\begin_inset Formula $L_{2}$
\end_inset

 distance for a successful Carlini Wagner 
\begin_inset Formula $L_{2}$
\end_inset

 targeted attack (higher is better 
\begin_inset Formula $\uparrow$
\end_inset

).
\begin_inset CommandInset label
LatexCommand label
name "tab:imagenet_evaluation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

Image classification evaluation results are shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:imagenet_evaluation}
\end_layout

\end_inset

, examples of successful attacks are shown in Figures
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:untargeted_examples}, 
\backslash
ref{fig:targeted_examples}
\end_layout

\end_inset

.
 The empirical results presented in Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:imagenet_evaluation}
\end_layout

\end_inset

 confirm that while VIB reduces performance on the validation set, it substantia
lly improves robustness to adversarial attacks.
 Moreover, these results demonstrate that VUB significantly outperforms
 VIB in terms of validation accuracy while providing competetive robustness
 to attacks similarly to VIB.
\end_layout

\begin_layout Standard
VUB allows us to easily estimate rate and distortion in training and plot
 an estimated information curve for real world problems.
 DNNs trained with VUB reproduced similar information plane behavior previously
 demonstrated on toy problems by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

 for both error minimization and representation compression phases, as shown
 in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{estimated_info_plane}
\end_layout

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../aaai/media/vub_info_plane.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated information plane metrics per epoch for VUB with 
\begin_inset Formula $\beta=0.01$
\end_inset

.
 
\begin_inset Formula $I(Z;X)$
\end_inset

 is approximated by 
\begin_inset Formula $H(R)-H(Z|X)$
\end_inset

 and 
\begin_inset Formula $\frac{1}{CH(Y;\hat{Y})}$
\end_inset

 is used as an analog for 
\begin_inset Formula $I(Z;Y)$
\end_inset

.
 The epochs have been grouped and color-coded in intervals of 25 epochs
 in the order: Orange (0-25), gray (26-50), red (51-75) and green (76-100).
\begin_inset CommandInset label
LatexCommand label
name "estimated_info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../aaai/media/untargeted_attack_examples_named.png
	scale 16

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Successful untargeted FGS attack examples.
 Images are perturbations of previously successfully classified instances
 from the ImageNet validation set.
 Perturbation magnitude is determined by the parameter 
\begin_inset Formula $\epsilon$
\end_inset

 shown on the left, the higher the more perturbed.
 Notice the deterioration of image quality as 
\begin_inset Formula $\epsilon$
\end_inset

 increases.
 Original and wrongly assigned labels are listed at the top of each image.
\begin_inset CommandInset label
LatexCommand label
name "fig:untargeted_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../aaai/media/targeted_cw_attack_examples_named.png
	scale 28

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Successful targeted CW attack examples.
 Images are perturbations of previously successfully classified instances
 from the ImageNet validation set.
 The target label is 'Soccer ball'.
 Average 
\begin_inset Formula $L_{2}$
\end_inset

 distance required for a successful attack is shown on the left.
 The higher the required 
\begin_inset Formula $L_{2}$
\end_inset

 distance the greater the visible change required to fool the model.
 Original and wrongly assigned labels are listed at the top of each image.
 Mind the difference in noticeable change as compared to FGS perturbations
 and between VIB and VUB perturbations.
\begin_inset CommandInset label
LatexCommand label
name "fig:targeted_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Text classification
\end_layout

\begin_layout Standard
A fine tuned BERT uncased 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Devlin2019}
\end_layout

\end_inset

 base model was used and achieved a 95.5% accuracy on the
\series bold
 
\series default
IMDB sentiment analysis test set.
 Each model was trained for 150 epochs and the first 200 entries in the
 test set used for evaluation and adversarial attacks.
\end_layout

\begin_layout Subsubsection
Evaluation and analysis
\end_layout

\begin_layout Standard
\begin_inset space ~
\end_inset


\end_layout

\begin_layout Standard
Text classification evaluation results are shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_evaluation}
\end_layout

\end_inset

, examples of successful attacks are shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_examples}
\end_layout

\end_inset

.
 In this modality VUB significantly outperforms VIB in both test set accuracy
 and robustness to both attacks.
 A comparison of the best VIB and VUB models further substantiates these
 findings, with statistical significance confirmed by a p-value of less
 than 0.05 in a Wilcoxon rank sum test.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="4">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top" width="1.9cm">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Test
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
DWB
\begin_inset Formula $\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
PWWS
\begin_inset Formula $\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vanilla model
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
95.5%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.9%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
100%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VIB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
91.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
35.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.4\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
41.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm6.6\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
90.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
41.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.8\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm14.3\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
89.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.9\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
90.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm8.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
99.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.9\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VUB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
93.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
27.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
28.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.3\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
92.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.8\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
30.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
50.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.8\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
89.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
99.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
100%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Evaluation for vanilla, VIB and VUB models, average over 5 runs with standard
 deviation over the IMDB dataset.
 First column is performance on the test set, second is % of successful
 Deep Word Bug attacks (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

), third column is % of successful PWWS attacks (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

).
 
\begin_inset CommandInset label
LatexCommand label
name "tab:text_attack_evaluation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="1">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top" width="8cm">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Original text
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\emph on
great
\emph default
 historical movie, will not allow a viewer to leave once you begin to watch.
 View is presented differently than displayed by most school books on this
 
\emph on
subject
\emph default
.
 My only fault for this movie is it was photographed in black and white;
 wished it had been in color ...
 wow !
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Perturbed text
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\emph on
gnreat
\emph default
 historical movie, will not allow a viewer to leave once you begin to watch.
 View is presented differently than displayed by most school books on this
 
\emph on
sSbject
\emph default
.
 My only fault for this movie is it was photographed in black and white;
 wished it had been in color ...
 wow !
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Example of a successful Deep Word Bug attack on a vanilla Bert model fine
 tuned over the IMDB dataset.
 The original label is 'Positive sentiment'.
 Perturbations, marked in italic font, change the classification to 'Negative
 sentiment'.
 VUB and VIB classifiers are far less susceptible to these perturbations
 as shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_evaluation}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:text_attack_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:Discussion"

\end_inset


\end_layout

\begin_layout Standard
Our study strengthens the argument for using the Information Bottleneck
 combined with variational approximations to obtain robust models that can
 withstand adversarial attacks.
 By deriving tighter bounds on the IB functional, we demonstrate their utility
 as the Variational Upper Bound (VUB) objective for neural networks.
 We demonstrate that VUB outperforms the Variational Information Bottleneck
 (VIB) in terms of test accuracy while providing similar or superior robustness
 to adversarial attacks in challenging classification tasks of different
 modalities.
 We observe that both methods promote a disentangled latent space by using
 a stochastic factorized prior 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Chen2018}
\end_layout

\end_inset

, while KL regularization enforces clustering around a 0 mean which might
 increase latent smoothness.
 These traits can make it difficult for minor perturbations to significantly
 alter latent semantics.
 Furthermore, the enhanced results induced by classifier regularization
 not only reinforce previous studies on the ELBO function, which suggest
 that overly powerful decoders diminish the quality learned representations
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018}
\end_layout

\end_inset

, but also align with the confidence penalty proposed by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Pereyra2017}
\end_layout

\end_inset

.
 In light of these findings, we suggest that practitioners monitor the rate
 distortion ratio and validation set accuracy during training.
 These metrics may be more informative indicators of model performance,
 as validation set cross entropy could increase as the model becomes more
 calibrated.
\end_layout

\begin_layout Standard
In conclusion, while the IB and its variational approximations do not provide
 a complete theoretical framework for DNN regularization, they offer a strong,
 measurable, and theoretically-grounded approach.
 VUB is presented as a tractable upper bound of the IB functional and a
 combination of previously derived non-IB regularization method.
 VUB can be easily adapted to any classifier DNN, including transformer
 based text classifiers, to significantly increase robustness to various
 adversarial attacks while inflicting minimal decrease in performence.
\end_layout

\begin_layout Standard
As a side note, this work presents, for the first time to the best of our
 knowledge, the observation of information plane error minimization and
 compression in real-world problems.
\end_layout

\begin_layout Standard
This study opens many opportunities for further research.
 Besides further improvements to the upper bound, it is intriguing to use
 VUB in self-supervised learning and in generative tasks.
 Other possible directions, including producing calibrated models as proposed
 by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Achille2018}
\end_layout

\end_inset

 are left for future work.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bibliography{draft.bib}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
