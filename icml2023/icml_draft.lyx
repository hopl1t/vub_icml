#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Tighter Bounds on the Information Bottleneck with Application to Deep Learning}

\bibliographystyle{icml2023}
\end_preamble
\options letterpaper
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
twocolumn[ 
\backslash
icmltitle{Tighter Bounds on the Information Bottleneck with Application
 to Deep Learning}
\end_layout

\begin_layout Plain Layout


\backslash
icmlsetsymbol{equal}{*}
\end_layout

\begin_layout Plain Layout


\backslash
begin{icmlauthorlist} 
\backslash
icmlauthor{Nir Weingarten}{runi} 
\backslash
icmlauthor{Zohar Yakhini}{runi} 
\backslash
icmlauthor{Moshe Butman}{runi} 
\backslash
icmlauthor{Ran Gilad-Bachrach}{tau} 
\backslash
end{icmlauthorlist}
\end_layout

\begin_layout Plain Layout


\backslash
icmlaffiliation{runi}{Efi Arzi School of Computer Science, Reichman University,
 Herzliya, Israel} 
\backslash
icmlaffiliation{tau}{Department of Biomedical Engineering, Tel-Aviv University,
 Tel-Aviv, Israel}
\end_layout

\begin_layout Plain Layout


\backslash
icmlcorrespondingauthor{Nir Weingarten}{nir.weingarten@runi.ac.il}
\end_layout

\begin_layout Plain Layout

% You may provide any keywords that you % find helpful for describing your
 paper; these are used to populate % the "keywords" metadata in the PDF
 but will not be shown in the document 
\backslash
icmlkeywords{Information Bottleneck, Information Theory, Representation
 Learning, Regularization, Robustness, Adversarial Attacks}
\end_layout

\begin_layout Plain Layout


\backslash
vskip 0.3in ]
\end_layout

\begin_layout Plain Layout

% This command actually creates the footnote in the first column
\end_layout

\begin_layout Plain Layout


\backslash
printAffiliationsAndNotice{}
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Deep Neural Nets (DNNs) learn latent representations induced by their downstream
 task, objective function, and other parameters.
 The quality of data modeling impacts the DNN's generalization ability and
 the coherence of the emerging latent space.
 The Information Bottleneck (IB) provides a hypothetically optimal framework
 for data modeling, but it is often intractable.
 Recent efforts combined deep learning with the IB using VAE-inspired variationa
l approximations to compute mutual information.
 This work introduces a new and tighter variational upper bound for the
 IB.
 When used as an objective function, it improves the performance of IB-inspired
 DNNs in terms of test accuracy while providing similar or superior robustness
 to adversarial attacks.
 These advancements strengthen the case for the IB and its variational approxima
tions as a framework for better representation learning, and provides a
 simple method to significantly enhance the adversarial robustness of any
 classifier DNN while suffering only a slight decrease in classification
 accuracy.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In recent years deep neural nets have gained increasing popularity in different
 learning tasks.
 Their ability to approximate complicated functions has revolutionized many
 computational fields.
 Despite the great achievements, it is still postulated that the current
 networks are prone to overfit the training data 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Ying2019}
\end_layout

\end_inset

, may be considerably uncalibrated 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Guo2017}
\end_layout

\end_inset

 and are susceptible to adversarial attacks 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Goodfellow2015}
\end_layout

\end_inset

.
 A question emerges regarding the extraction of an optimal representation
 for all data points from our restricted set of training examples.
 Classic information theory provides tools to optimize compression and transmiss
ion of data, but it does not provide methods to gauge the relevance of the
 compressed signal to downstream tasks.
 Methods such as rate distortion 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Blahut1972}
\end_layout

\end_inset

 regard all information as equal, not taking into account which information
 is more relevant without constructing complex distortion functions.
 To resolve these limitations, the Information Bottleneck, IB 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

, defines an information theoretic limit for the rate distortion ratio of
 any learning task given a Lagrangian hyper parameter 
\begin_inset Formula $\beta$
\end_inset

 controlling the tradeoff between the rate and the distortion.
 Optimizing a learning task using the IB objective results in an encoding
 with optimal rate distortion ratio for a downstream task, over a chosen
 Lagrangian 
\begin_inset Formula $\beta$
\end_inset

 and for a mutual information distortion function.
 Computing the IB requires mutual information calculations which are tractable
 in discrete settings and for some specific continuous distributions.
 Adopting the IB framework for DNNs requires computing mutual information
 for unknown distributions and has no analytic solution.
 However, recent work approximated tractable upper bounds for the IB functional
 in DNN settings using variational approximations.
 Variational auto encoders 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

 use stochastic DNNs to approximate intractable and unknown distributions.
 Assuming a latent variable model the marginal 
\begin_inset Formula $p(x)=\int p(x|z)p(z)dz$
\end_inset

 (Where 
\begin_inset Formula $X$
\end_inset

 is an observed variable and 
\begin_inset Formula $Z$
\end_inset

 is unobserved) can be approximated using a chosen variational distribution
 optimized to fit the training data.
 This optimization is possible using Stochastic Gradient Descent (SGD) and
 the 'reparameterization trick' as elaborated in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:variational_approximations"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Similarly to VAEs 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

 proposed using stochastic DNNs as variational approximations for a latent
 model thus making possible the computations of upper bounds for mutual
 information between the DNN's input, output and hidden layers.
 A proposed optimization method called VIB - 'Deep Variational Information
 Bottleneck' derives an upper bound for the IB objective and minimizes it's
 approximation using Monte Carlo sampling over training data.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

 found that replacing a DNN's deterministic classifier layer with a stochastic
 layer optimized with the VIB objective results in a slight decrease in
 test set accuracy but a significant increase in robustness to adversarial
 attacks in complicated classification tasks.
 
\end_layout

\begin_layout Standard
While providing a complete framework for optimal data modeling, the IB,
 and it's variational approximations, rely on three assumptions: (1) It
 suffices to optimize the mutual information metric to optimize a model's
 performance; (2) Forgetting more information about the input while keeping
 the same information about the output induces better generalization; (3)
 Mutual information between the input, output and latent representation
 can be either computed or approximated to a desired level of accuracy.
\end_layout

\begin_layout Standard
In this study, we adopt the same information theoretic and variational approach
 used previously.
 The work begins by deriving an upper bound for the IB functional.
 We then employ a tractable variational approximation for this bound, named
 VUB - 'Variational Upper Bound' and show that it is a tighter bound on
 IB than VIB.
 We proceed to show empirical evidence that VUB substantially increases
 test accuracy over VIB while providing similar or superior robustness to
 adversarial attacks across several challenging tasks and different modalities.
 Finally, we discuss these effects in the context of previous work on the
 IB and other DNN regularization techniques.
 The conclusion drawn is that while more mutual information between encoding
 and output does not necessarily improve classification, and more compressed
 encoding does not always enhance regularization 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Amjad2020}
\end_layout

\end_inset

, the application of IB approximations as objectives to DNNs empirically
 improves regularization, suggesting better data modeling.
 This notion contributes for the adaptation of the IB, and it's variational
 approximations, as an objective to learning tasks and as a theoretic framework
 to gauge and explain data modeling.
\end_layout

\begin_layout Standard
In addition, we demonstrate that VUB can be easily adapted to any classifier
 DNN, including transformer based NLP classifiers, to substantially increase
 robustness to adversarial attacks while only slightly decreasing test accuracy.
\end_layout

\begin_layout Subsection
Preliminaries
\end_layout

\begin_layout Standard
The following literature review and derivations refer to information theory
 and variational approximations.
 A preliminary mutual ground and notation follows:
\end_layout

\begin_layout Standard
We denote random variables with upper cased letters 
\begin_inset Formula $X,Y$
\end_inset

, and their realizations in lower case 
\begin_inset Formula $x,y$
\end_inset

.
 Denote discrete Probability Mass Functions (PMFs) with an upper case 
\begin_inset Formula $P(X)$
\end_inset

 and continuous Probability Density Functions (PDFs) with a lower case 
\begin_inset Formula $p(x)$
\end_inset

.
 Hat notation denotes empirical measurements.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X,Y$
\end_inset

 be two observed random variables (RVs) with unknown distributions 
\begin_inset Formula $p^{*}(x),p^{*}(y)$
\end_inset

 that we aim to model.
 Assume 
\begin_inset Formula $X,Y$
\end_inset

 are governed by some unknown underlying process with a joint probability
 distribution 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

.
 We can attempt to approximate these distributions using a model 
\begin_inset Formula $p_{\theta}$
\end_inset

 with parameters 
\begin_inset Formula $\theta$
\end_inset

 such that for generative tasks 
\begin_inset Formula $p_{\theta}(x)\approx p^{*}(x)$
\end_inset

 and for discriminative tasks 
\begin_inset Formula $p_{\theta}(y|x)\approx p^{*}(y|x)$
\end_inset

, using a dataset 
\begin_inset Formula $\mathcal{S}=\left\{ (x_{1},y_{1}),...,(x_{N},y_{N})\right\} $
\end_inset

 to fit our model.
 One can also assume the existence of an additional unobserved RV 
\begin_inset Formula $Z\sim p^{*}(z)$
\end_inset

 that influences or generates the observed RVs 
\begin_inset Formula $X,Y$
\end_inset

.
 Since 
\begin_inset Formula $Z$
\end_inset

 is unobserved it is absent from the dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

 and so cannot be modeled directly.
 Denote 
\begin_inset Formula $\int p_{\theta}(x|z)p_{\theta}(z)dz$
\end_inset

 the marginal, 
\begin_inset Formula $p_{\theta}(z)$
\end_inset

 the prior as it is not conditioned over any other RV, and 
\begin_inset Formula $p_{\theta}(z|x)$
\end_inset

 the posterior following Bayes' rule.
 Latent models are frequently used for creating variational bounds on mutual
 information as they make good approximations for 
\begin_inset Formula $p^{*}(x)$
\end_inset

 due to their high expressivity 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2019}
\end_layout

\end_inset

, as is demonstrated in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:variational_approximations"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Deep neural nets, or DNNs, are a powerful tool to approximate complicated
 functions such as modeling 
\begin_inset Formula $p_{\theta}(y|x)$
\end_inset

 or 
\begin_inset Formula $p_{\theta}(x)$
\end_inset

.
 When choosing a DNN to approximate a latent model we encounter a problem
 as the marginal integral 
\begin_inset Formula $p_{\theta}(x)=\int p_{\theta}(x,z)dz$
\end_inset

 doesn't have an analytic solution and hence not optimizable over gradient
 descent 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2019}
\end_layout

\end_inset

.
 This intractability can be overcome by using a tractable parametric variational
 encoder 
\begin_inset Formula $q_{\phi}(z|x)$
\end_inset

 to approximate the posterior 
\begin_inset Formula $p_{\theta}(z|x)$
\end_inset

 such that 
\begin_inset Formula $q_{\phi}(z|x)\approx p_{\theta}(z|x)$
\end_inset

, and estimate 
\begin_inset Formula $p_{\theta}(x,z)$
\end_inset

 or 
\begin_inset Formula $p_{\theta}(x,z|y)$
\end_inset

 using Monte Carlo sampling from the dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

 during optimization.
\end_layout

\begin_layout Standard
In this work information theoretic functions share the same notation for
 discrete and continuous settings and are denoted as follows:
\end_layout

\begin_layout Standard
\align left
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="middle" width="2.5cm">
<column alignment="left" valignment="middle" width="5cm">
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $H_{p}(X)=-\int p(x)log\left(p(x)\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cross
\end_layout

\begin_layout Plain Layout
entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $CE(P,Q)=-\int p(x)log\left(q(x)\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
KL
\end_layout

\begin_layout Plain Layout
divergence
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $D_{KL}\left(P\Big|\Big|Q\right)=\int p(x)log\left(\frac{p(x)}{q(x)}\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mutual information
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $I(X;Y)=\int\int p(x,y)log\left(\frac{p(x,y)}{p(x)p(y)}\right)dxdy$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Section
Related work
\end_layout

\begin_layout Subsection
IB and it's analytic solutions
\end_layout

\begin_layout Standard
Classic information theory offers rate-distortion 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Blahut1972}
\end_layout

\end_inset

 to mitigate signal loss during compression.
 Rate being the signal's compression measured by mutual information between
 input and output signals, and distortion a chosen task-specific function.
 The Information Bottleneck method extends rate-distortion by replacing
 the tailored distortion functions with mutual information over a target
 distribution.
 This allows optimizing a signal's compression to any chosen downstream
 task.
 Denote 
\begin_inset Formula $X$
\end_inset

 the source signal, 
\begin_inset Formula $Z$
\end_inset

 it's encoding and 
\begin_inset Formula $Y$
\end_inset

 the target signal for some specific downstream task.
 Let 
\begin_inset Formula $T$
\end_inset

 be a positive minimal threshold for the desired distortion and assume a
 latent variable model with the Markov chain 
\begin_inset Formula $Z\leftrightarrow X\leftrightarrow Y$
\end_inset

 and define the distortion function as mutual information between encoding
 and downstream task.
 We seek an optimal encoding 
\begin_inset Formula $Z:\underset{P(Z|X)}{min}I(X;Z)$
\end_inset

 subject to 
\begin_inset Formula $I(Z;Y)\ge T$
\end_inset

.
 This constrained problem can be implicitly optimized by minimizing the
 functional 
\begin_inset Formula $L_{P(Z|X)}=I(Z;X)-\beta I(Z;Y)$
\end_inset

 over the Lagrangian 
\begin_inset Formula $\beta$
\end_inset

, the first term being rate and the second distortion.
 The optimal solution is a function of 
\begin_inset Formula $\beta$
\end_inset

 and was named 'the information curve' as shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../aaai/media/info_plane.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The information plane and curve - rate distortion ratio over 
\begin_inset Formula $\beta$
\end_inset

.
 Adapted from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Slonim2002}
\end_layout

\end_inset

.
 At 
\begin_inset Formula $\beta=0$
\end_inset

, representation is compressed but uninformative (maximal compression),
 at 
\begin_inset Formula $\beta\rightarrow\infty$
\end_inset

 it's informative but potentially overfitted (maximal information).
\begin_inset CommandInset label
LatexCommand label
name "info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The IB functional is only tractable when mutual information can be computed
 and was demonstrated by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Tishby1999}
\end_layout

\end_inset

 for soft clustering tasks over a discrete and known distribution 
\begin_inset Formula $P^{*}(x,y)$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Chechik2003}
\end_layout

\end_inset

 extended IB for gaussian distributions and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Painsky2017}
\end_layout

\end_inset

 offered a limited linear approximation of IB for any distribution.
\end_layout

\begin_layout Subsection
IB and deep learning
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Tishby2015}
\end_layout

\end_inset

 proposed an IB interpretation of DNNs regarding them as Markov cascades
 of intermediate representations between hidden layers.
 Neural architecture and dataset cardinality are theoretically sufficient
 to compute the optimal rate distortion of a DNN on the information curve.
 This suggests a model 
\emph on
complexity gap
\emph default
 between the achieved and optimal IB compression rate for that setting,
 and it is hypothesized that bridging this gap will result in optimal generaliza
tion.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

 visualized and analyzed the information plane behavior of DNNs over a toy
 problem with a known joint distribution.
 Mutual information of the different layers was estimated and used to analyze
 the training process.
 SGD exhibited two separate and sequential behaviors during training: A
 short empirical error minimization phase (ERM) characterized by larger
 gradient norms and a rapid decrease in distortion, followed by a long compressi
on phase with smaller gradient norms and an increase in rate until convergence
 to an optimal IB limit as demonstrated in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{shwartz_ziv_info_plane}
\end_layout

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../../aaai/media/black_box.png
	scale 36

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Information plane scatters of different DNN layers (colors) in 50 randomized
 networks.
 From 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.
 Left are initial weights, Right are at 400 epochs.
 
\begin_inset Formula $I(T;Y),I(X;T)$
\end_inset

 are analogous to 
\begin_inset Formula $I(Z;Y),I(X;Z)$
\end_inset

 in the current study.
 We believe to be the first to demonstrate similar information plane behavior
 on real-world problems as shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{estimated_info_plane}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "shwartz_ziv_info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Amjad2020}
\end_layout

\end_inset

 pointed out three flaws in the usage of the IB functional as an objective
 for deterministic DNN classifiers: (1) When data 
\begin_inset Formula $X$
\end_inset

 is absolutely continuous the mutual information term 
\begin_inset Formula $I(X;Z)$
\end_inset

 is infinite; (2) When data 
\begin_inset Formula $X$
\end_inset

 is discrete the IB functional is a piecewise constant function of the parameter
s, making it's SGD optimization difficult or impossible; (3) Equivalent
 representations might yield the same IB loss while one achieved much better
 classification rate than the other.
 These discrepancies were attributed to to mutual information's invariance
 to invertible transformations and the absence of the decision function
 in the objective.
\end_layout

\begin_layout Subsection
Variational approximations to the IB objective
\begin_inset CommandInset label
LatexCommand label
name "subsec:variational_approximations"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Kingma2014}
\end_layout

\end_inset

 introduced the variational auto encoder - a stochastic generative DNN.
 An unobserved RV 
\begin_inset Formula $Z$
\end_inset

 is assumed to generate evidence 
\begin_inset Formula $X,Y$
\end_inset

 and the true probability 
\begin_inset Formula $p^{*}(x)$
\end_inset

 can be modeled using a parametric model over the marginal 
\begin_inset Formula $p_{\theta}(x)=\int p_{\theta}(x|z)p_{\theta}(z)dz$
\end_inset

.
 However, since the marginal is intractable a variational approximation
 
\begin_inset Formula $q_{\phi}(z|x)\approx p_{\theta}(z|x)$
\end_inset

 is proposed instead.
 The log probability 
\begin_inset Formula $log\left(p_{\theta}(x)\right)$
\end_inset

 is then developed in to the tractable VAE loss comprised of the Evidence
 Lower Bound (ELBO) and KL regularization terms: 
\begin_inset Formula $\underset{\text{\emph{\text{\emph{ELBO}}}}}{\mathcal{L}}=\mathbb{E}_{q_{\phi}(z|x)}\left[log\left(p_{\theta}(x|z)\right)\right]-D_{KL}\left(q_{\phi}(z|x)\Big|\Big|p_{\theta}(z)\right)$
\end_inset

.
 
\begin_inset Formula $q_{\phi}(z|x)$
\end_inset

 is modeled using a stochastic neural encoder having it's final activation
 used as parameters for the assumed variational distribution (typically
 a spherical gaussian with parameters 
\begin_inset Formula $\mu,\Sigma$
\end_inset

).
 Each forward pass emulates a stochastic realization 
\begin_inset Formula $z\in Z$
\end_inset

 from these parameters by using the 'reparameterization trick': 
\begin_inset Formula $z=\mu+\epsilon\cdot\Sigma$
\end_inset

 for some unparameterized scalar 
\begin_inset Formula $\epsilon\sim N(0,1)$
\end_inset

 s.t.
 a backwards pass is possible.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Higgins2017}
\end_layout

\end_inset

 later proposed the 
\begin_inset Formula $\beta$
\end_inset

-autoencoder introducing a hyper parameter 
\begin_inset Formula $\beta$
\end_inset

 over the KL term to control the regularization-reconstruction tradeoff.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

 introduced the Variational Information Bottleneck (VIB) as a variational
 approximation for an upper bound to the IB objective for classifier DNN
 optimization: Upper bounds for 
\begin_inset Formula $I(Z,Y),I(X,Z)$
\end_inset

 are derived from the identity 
\begin_inset Formula $D_{KL}\left(\text{True probability}\Big|\Big|\text{Variational approximation}\right)\ge0$
\end_inset

 and used to form an upper bound for the IB functional.
 This upper bound is approximated using variational approximations for 
\begin_inset Formula $p^{*}(y|z),p^{*}(z)$
\end_inset

 similarly to VAEs.
 This approximation to an upper bound over the IB objective is empirically
 estimated as Cross entropy and a beta scaled KL regularization term as
 in 
\begin_inset Formula $\beta$
\end_inset

-autoencoders and is optimized over the training data 
\begin_inset Formula $\mathcal{S}$
\end_inset

 using Monte Carlo sampling and the reparameterization trick.
 VIB was evaluated over MNIST and ImageNet and, while causing a slight reduction
 in test-accuracy, it generated substantial improvements in robustness to
 adversarial attacks.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 found that as the ELBO loss in VAEs depends solely on image reconstruction
 it does not necessarily induce a better quality modeling of the marginal
 
\begin_inset Formula $p_{\theta}(z)$
\end_inset

, hence not necessarily a better representation learned.
 This gap is attributed to powerful decoders being overfitted.
 It is suggested to keep 
\begin_inset Formula $\beta$
\end_inset

 values under 
\begin_inset Formula $1$
\end_inset

 and monitor the rate-distortion tradeoff as well as cross-entropy loss.
\end_layout

\begin_layout Standard
Additional noteworthy contributions to this field have been made in recent
 years by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Achille2018,Wieczorek2019,Fischer2020}
\end_layout

\end_inset

 and others.
 However, a detailed review of these works is beyond the scope of this paper.
\end_layout

\begin_layout Subsection
Non IB information theoretic regularization
\end_layout

\begin_layout Standard
Label smoothing 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Szegedy2016}
\end_layout

\end_inset

 and entropy regularization 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Pereyra2017}
\end_layout

\end_inset

 both regularize classifier DNNs by increasing the entropy of their output.
 This is done either directly with an entropy term in the loss function,
 
\begin_inset Formula $\beta\cdot H\left(p_{\theta}(y|x)\right),$
\end_inset

 or by smoothing the training data labels.
 Applying both methods was demonstrated to improve test accuracy and model
 calibration.
 In the current work a similar conditional entropy term emerges from the
 derivation of the new upper bound for the IB objective as shown in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:vub"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Section
From VIB to VUB
\begin_inset CommandInset label
LatexCommand label
name "sec:vub"

\end_inset


\end_layout

\begin_layout Standard
The VIB loss 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017}
\end_layout

\end_inset

 consists of a Cross-Entropy (
\begin_inset Formula $CE$
\end_inset

) term and a KL regularization term, similar to the VAE loss.
 The KL term is derived from a bound on the IB rate term 
\begin_inset Formula $I(X;Z)$
\end_inset

, while the 
\begin_inset Formula $CE$
\end_inset

 term from a bound on the IB distortion term 
\begin_inset Formula $I(Z;Y)=H(Y)-H(Y|Z)$
\end_inset

.
 When deriving the distortion term the entropy term 
\begin_inset Formula $H(Y)$
\end_inset

 is ignored as it is constant and does not effect optimization, while the
 conditional entropy term 
\begin_inset Formula $H(Y|Z)$
\end_inset

 is derived into a cross entropy term.
 We note that since 
\begin_inset Formula $Y$
\end_inset

 is unknown any optimization on 
\begin_inset Formula $Z$
\end_inset

, including 
\begin_inset Formula $CE$
\end_inset

, depends on our decoder model of 
\begin_inset Formula $Y$
\end_inset

.
 Following this logic we reintroduce the omitted 
\begin_inset Formula $H(Y)$
\end_inset

 term into the objective.
 We replace the unknown 
\begin_inset Formula $H(Y)$
\end_inset

 with a variational approximation of the decoder's entropy, which provides
 a lower bound.
\end_layout

\begin_layout Subsection
IB upper bound
\end_layout

\begin_layout Standard
We begin by establishing a new upper bound for the IB functional, starting
 with the same derivation as shown in VIB.
\end_layout

\begin_layout Standard
We begin by bounding 
\begin_inset Formula $I(Z;X)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;X)= & \int\int p^{*}(x,z)log\left(p^{*}(z|x)\right)dxdz\nonumber \\
- & \int p^{*}(z)log\left(p^{*}(z)\right)dz\label{eq:i_z_x1}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For any probability distribution 
\begin_inset Formula $r$
\end_inset

 we have that 
\begin_inset Formula $D_{KL}\left(p^{*}(z),r(z)\right)\ge0$
\end_inset

, it follows that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\int p^{*}(z)log\left(p^{*}(z)\right)dz\ge\int p^{*}(z)log\left(r(z)\right)dz\label{eq:d_kl}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And so:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
I(Z;X)\overset{\ref{eq:d_kl}}{\le}\int\int p^{*}(x)p^{*}(z|x)log\left(\frac{p^{*}(z|x)}{r(z)}\right)dxdz\label{eq:i_z_x2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We proceed by bounding 
\begin_inset Formula $I(Z;Y)$
\end_inset

:
\end_layout

\begin_layout Standard
For any probability distribution 
\begin_inset Formula $c$
\end_inset

 we have that 
\begin_inset Formula $D_{KL}\left(p^{*}(y|z),c(y|z)\right)\ge0$
\end_inset

, it follows that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\int p^{*}(y|z)log\left(p^{*}(y|z)\right)dy\ge\int p^{*}(y|z)log\left(c(y|z)\right)dy\label{eq:d_kl2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And so:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;Y)= & \int\int p^{*}(y,z)log\left(\frac{p^{*}(y,z)}{p^{*}(y)p^{*}(z)}\right)dydz\nonumber \\
\overset{\text{\ref{eq:d_kl2}}}{\ge} & \int\int p^{*}(y|z)p^{*}(z)log\left(\frac{c(y|z)}{p^{*}(y)}\right)dydz\nonumber \\
\overset{\text{}}{=} & \int\int p^{*}(y,z)log\left(c(y|z)\right)dydz+H_{p^{*}}(Y)\label{eq:i_z_y}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
We continue bounding 
\begin_inset Formula $I(Z;Y)$
\end_inset

 without discarding the entropy of 
\begin_inset Formula $Y$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\ge & \int\int p^{*}(y,z)log\left(c(y|z)\right)dydz\nonumber \\
+ & min\left\{ H_{p^{*}}(Y),H_{c}(Y|Z)\right\} \label{eq:i_z_y2}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
We further develop this term using the markov chain 
\begin_inset Formula $Z\leftarrow X\leftarrow Y$
\end_inset

 and total probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
 & I(Z;Y)\ge\nonumber \\
 & \int\int\int p^{*}(x)p^{*}(y|x)p^{*}(z|x)log\left(c(y|z)\right)dxdydz\nonumber \\
- & min\left\{ H_{p^{*}}(Y),-\int\int c(y,z)log\left(c(y|z)\right)dydz\right\} \label{eq:i_z_y3}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
We define 
\begin_inset Formula $L_{UB}$
\end_inset

: A new upper bound for the IB functional.
 
\begin_inset Formula $L_{UB}$
\end_inset

 is composed of the new bound on the distortion term derived in Equation
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:i_z_y3}
\end_layout

\end_inset

 together with the previously bound on the rate term derived in Equation
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:i_z_x2}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
 & L_{UB}\equiv\nonumber \\
 & \int\int p^{*}(x)p^{*}(z|x)log\left(\frac{p^{*}(z|x)}{r(z)}\right)dxdz\nonumber \\
- & \int\int\int p^{*}(x)p^{*}(y|x)p^{*}(z|x)log\left(c(y|z)\right)dxdydz\nonumber \\
+ & min\left\{ H_{p^{*}}(Y),-\int\int c(y,z)log\left(c(y|z)\right)dydz\right\} \label{eq:l_ub}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
It is easy to verify that the bound holds for all 
\begin_inset Formula $\beta\ge0$
\end_inset

 such that 
\begin_inset Formula $L_{IB}=\beta\cdot I\left(Z;X\right)-I\left(Z;Y\right)$
\end_inset

.
\end_layout

\begin_layout Subsection
Variational approximation
\end_layout

\begin_layout Standard
We follow the same variational approach as in VIB.
 We define 
\begin_inset Formula $L_{VUB}$
\end_inset

 as a new tractable upper bound for the IB functional.
 Let 
\begin_inset Formula $p^{*}(x,y,z)$
\end_inset

 be the unknown joint distribution, 
\begin_inset Formula $e(z|x)$
\end_inset

 a variational encoder approximating 
\begin_inset Formula $p^{*}(z|x)$
\end_inset

, 
\begin_inset Formula $c(y|z)$
\end_inset

 a variational classifier approximating 
\begin_inset Formula $p^{*}(y|z)$
\end_inset

.
 We reintroduce 
\begin_inset Formula $\beta$
\end_inset

 to allow tuning of the KL term and replace the intractable 
\begin_inset Formula $p^{*}(z)$
\end_inset

 is with the variational approximation 
\begin_inset Formula $r(z)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
 & L_{VUB}\equiv\nonumber \\
 & \beta\int\int p^{*}(x)e_{\phi}(z|x)log\left(\frac{e_{\phi}(z|x)}{r(z)}\right)dxdz\nonumber \\
- & \int\int\int p^{*}(x)p^{*}(y|x)e_{\phi}(z|x)log\left(c_{\lambda}(y|z)\right)dxdydz\label{eq:l_vub}\\
- & min\Bigg\{ H_{p^{*}}(Y),\nonumber \\
 & -\int\int\int p^{*}(x)e_{\phi}(z|x)c_{\lambda}(y|z)log\left(c_{\lambda}(y|z)\right)dxdydz\Bigg\}\nonumber \\
\ge & L_{IB}\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Empirical estimation
\end_layout

\begin_layout Standard
We proceed to model VUB using DNNs and optimize it using Monte Carlo sampling
 over the training data 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
 Let 
\begin_inset Formula $e_{\phi}$
\end_inset

 be a stochastic DNN encoder with parameters 
\begin_inset Formula $\phi$
\end_inset

 applying the reparameterization trick such that 
\begin_inset Formula $e_{\phi}(x)\sim N(\mu,\Sigma)$
\end_inset

.

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 Let 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $C_{\lambda}$
\end_inset

 be a discrete classifier DNN parameterized by 
\begin_inset Formula $\lambda$
\end_inset

 such that 
\begin_inset Formula $C_{\lambda}(\hat{z})\sim Categorical$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\hat{L}_{VUB} & \equiv\nonumber \\
 & \frac{1}{N}\sum_{n=1}^{N}\Bigg[\beta\cdot D_{KL}\left(e_{\phi}(x_{n})\Bigg|\Bigg|r(z)\right)\label{eq:l_vub_empirical}\\
- & P^{*}(y_{n})\cdot log\left(C_{\lambda}\left(e_{\phi}(x_{n})\right)\right)\nonumber \\
- & min\left\{ H(\hat{Y}),H\left(C_{\lambda}\left(e_{\phi}(x_{n})\right)\right)\right\} \Bigg]\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
As in VIB and VAE 
\begin_inset Formula $e_{\phi}(x)$
\end_inset

 is a computed as spherical gaussian by using the first half of the encoder's
 output entries as 
\begin_inset Formula $\mu$
\end_inset

 and the second as the diagonal 
\begin_inset Formula $\Sigma$
\end_inset

.
\end_layout

\begin_layout Subsection
Interpretation
\end_layout

\begin_layout Standard
VUB is in fact VIB regulated by a conditional entropy term 
\begin_inset Formula $-H(Y|Z)$
\end_inset

 similarly to the confidence penalty suggested by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Pereyra2017}
\end_layout

\end_inset

.
 Hence, VUB adds regularization over the classifier preventing it to overfit
 the embeddings.
 This is a possible remedy to the discrepancies in the ELBO loss observed
 by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
In terms of tightness we have that VUB is a tighter theoretical bound on
 IB than VIB for any 
\begin_inset Formula $Y$
\end_inset

 s.t.
 
\begin_inset Formula $H(Y)>0$
\end_inset

, and a tighter empirical bound for all 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Section
Experiments
\begin_inset CommandInset label
LatexCommand label
name "sec:Experiments"

\end_inset


\end_layout

\begin_layout Standard
We follow the experimental setup proposed by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

, extending it to NLP tasks as well.
 Image classification models were trained on the first 500,000 samples of
 the ImageNet 2012 dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{deng2009imagenet}
\end_layout

\end_inset

 and text classification over
\series bold
 
\series default
the IMDB
\series bold
 
\series default
sentiment analysis dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{maas2011imdb}
\end_layout

\end_inset

.
 For each dataset, a competitive pre-trained model (Vanilla model) was evaluated
 and then used to encode embeddings.
 These embeddings were then used as a dataset for a new stochastic classifier
 net with either a VIB or a VUB loss function.
 Stochastic classifiers consisted of two ReLU activated linear layers of
 the same dimensions as the pre-trained model's logits (2048 for image and
 768 for text classification), followed by reparameterization and a final
 softmax activated FC layer.
 Learning rate was 
\begin_inset Formula $10^{-4}$
\end_inset

 and decaying exponentially with a factor of 0.97 every two epochs.
 Batch sizes were 32 for ImageNet and 16 for IMDB.
 We used a single forward pass per sample for inference.
 Each model was trained and evaluated 5 times per 
\begin_inset Formula $\beta$
\end_inset

 value with consistent performance.
 Beta values of 
\begin_inset Formula $\beta=10^{-i}$
\end_inset

 for 
\begin_inset Formula $i\in\{1,2,3\}$
\end_inset

 were tested since previous studies indicated this is the best range for
 VIB 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017,Alemi2018}
\end_layout

\end_inset

.
 Each model was evaluated using test set accuracy and robustness to various
 adversarial attacks over the test set.
 For image classification we employed the untargeted Fast Gradient Sign
 (FGS) attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Goodfellow2015}
\end_layout

\end_inset

 as well as the targeted CW 
\begin_inset Formula $L_{2}$
\end_inset

 optimization attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Carlini2017}, 
\backslash
cite{Kaiwen2018}
\end_layout

\end_inset

.
 For text classification we used the untargeted Deep Word Bug attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{gao2018deepwordbug}, 
\backslash
cite{Morris2020}
\end_layout

\end_inset

 as well as the untargeted PWWS attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{ren2019pwws}
\end_layout

\end_inset

.
 All models were trained using an Nvidia RTX3080 GPU.
 Code to reconstruct the experiments is provided in the code & data appendix.
\end_layout

\begin_layout Subsection
Image classification
\end_layout

\begin_layout Standard
A pre-trained inceptionV3 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Szegedy2016}
\end_layout

\end_inset

 base model was used and achieved a 77.21% accuracy on the image-net 2012
 validation set (Test set for image-net is unavailable).
 Note that inceptionV3 yields a slightly worse single shot accuracy than
 inceptionV2 (80.4%) when run in a single model and single crop setting,
 however we've used InceptionV3 over V2 for simplicity.
 Each model was trained for 100 epochs.
\end_layout

\begin_layout Subsubsection
Evaluation and analysis
\end_layout

\begin_layout Standard
\begin_inset Float table
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="5">
<features booktabs="true" tabularvalignment="middle" tabularwidth="8cm">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Val 
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\underset{\epsilon=0.1}{\text{FGS}}\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\underset{\epsilon=0.5}{\text{FGS}}\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
CW
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vanilla model
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
77.2%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
68.9%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
67.7%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
788
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VIB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
73.7%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
59.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
63.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3917
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm291$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
72.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
53.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3318
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm293$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
72.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.01\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
58.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3318
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm293$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VUB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.03\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
66.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
2666
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm140$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.05\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
57.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
64.3%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1564
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm218$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
74.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.09\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
57.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
64.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3575
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm456$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Image-net evaluation scores for vanilla, VIB and VUB models, average over
 5 runs with standard deviation.
 First column is performance on the image-net validation set (higher is
 better 
\begin_inset Formula $\uparrow$
\end_inset

).
 Second and third columns are the % of successful FGS attacks at 
\begin_inset Formula $\epsilon=0.1,0.5$
\end_inset

 (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

).
 Fourth column is the average 
\begin_inset Formula $L_{2}$
\end_inset

 distance for a successful Carlini Wagner 
\begin_inset Formula $L_{2}$
\end_inset

 targeted attack (higher is better 
\begin_inset Formula $\uparrow$
\end_inset

).
\begin_inset CommandInset label
LatexCommand label
name "tab:imagenet_evaluation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

Image classification evaluation results are shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:imagenet_evaluation}
\end_layout

\end_inset

, examples of successful attacks are shown in Figures
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:untargeted_examples}, 
\backslash
ref{fig:targeted_examples}
\end_layout

\end_inset

.
 The empirical results presented in Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:imagenet_evaluation}
\end_layout

\end_inset

 confirm that while VIB reduces performance on the validation set, it substantia
lly improves robustness to adversarial attacks.
 Moreover, these results demonstrate that VUB significantly outperforms
 VIB in terms of validation accuracy while providing competitive robustness
 to attacks similarly to VIB.
 A comparison of the best VIB and VUB models further substantiates these
 findings, with statistical significance confirmed by a p-value of less
 than 0.05 in a Wilcoxon rank sum test.
\end_layout

\begin_layout Standard
In addition to the evaluation metrics, we measured approximated rate and
 distortion throughout training and plotted them on the information curve
 as shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{estimated_info_plane}
\end_layout

\end_inset

.
 We notice recurring patterns of distortion reduction followed by rate increase,
 resembling the ERM and representation compression stages described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.

\series bold
 
\series default

\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../aaai/media/info_plane_imdb.png
	scale 31

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated information plane metrics per epoch for VUB trained on IMDB with
 
\begin_inset Formula $\beta=0.001$
\end_inset

.
 
\begin_inset Formula $I(Z;X)$
\end_inset

 is approximated by 
\begin_inset Formula $H(R)-H(Z|X)$
\end_inset

 and 
\begin_inset Formula $\frac{1}{CE(Y;\hat{Y})}$
\end_inset

 is used as an analog for 
\begin_inset Formula $I(Z;Y)$
\end_inset

.
 The epochs have been grouped and color-coded in intervals of 30 epochs
 in the order: Orange (0-30), gray (30-60), yellow (60-90), green (90-120)
 and red (120-150).
 We notice recurring patterns of distortion reduction followed by rate increase,
 resembling the ERM and representation compression stages described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "estimated_info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/nirweingarten/Desktop/university/idc/thesis/icml/media/two_images/targeted_two_images.png
	scale 38

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Successful targeted CW attack examples.
 Images are perturbations of previously successfully classified instances
 from the ImageNet validation set.
 The target label is 'Soccer ball'.
 Average 
\begin_inset Formula $L_{2}$
\end_inset

 distance required for a successful attack is shown on the left.
 The higher the required 
\begin_inset Formula $L_{2}$
\end_inset

 distance the greater the visible change required to fool the model.
 Original and wrongly assigned labels are listed at the top of each image.
 Mind the difference in noticeable change as compared to FGS perturbations
 and between VIB and VUB perturbations.
\begin_inset CommandInset label
LatexCommand label
name "fig:targeted_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Text classification
\end_layout

\begin_layout Standard
A fine tuned BERT uncased 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Devlin2019}
\end_layout

\end_inset

 base model was used and achieved a 95.5% accuracy on the
\series bold
 
\series default
IMDB sentiment analysis test set.
 Each model was trained for 150 epochs and the first 200 entries in the
 test set used for evaluation and adversarial attacks.
\end_layout

\begin_layout Subsubsection
Evaluation and analysis
\end_layout

\begin_layout Standard
Text classification evaluation results are shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_evaluation}
\end_layout

\end_inset

, examples of successful attacks are shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_examples}
\end_layout

\end_inset

.
 In this modality VUB significantly outperforms VIB in both test set accuracy
 and robustness to both attacks.
 A comparison of the best VIB and VUB models further substantiates these
 findings, with statistical significance confirmed by a p-value of less
 than 0.05 in a Wilcoxon rank sum test.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="4">
<features booktabs="true" tabularvalignment="middle" tabularwidth="8cm">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Test
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
DWB
\begin_inset Formula $\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
PWWS
\begin_inset Formula $\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vanilla model
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
95.5%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.9%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
100%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VIB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
91.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
35.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.4\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
41.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm6.6\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
90.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
41.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.8\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm14.3\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
89.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.9\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
90.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm8.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
99.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.9\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VUB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
93.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
27.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
28.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.3\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
92.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.8\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
30.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
50.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.8\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
89.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
99.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
100%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Evaluation for vanilla, VIB and VUB models, average over 5 runs with standard
 deviation over the IMDB dataset.
 First column is performance on the test set (higher is better 
\begin_inset Formula $\uparrow$
\end_inset

), second is % of successful Deep Word Bug attacks (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

), third column is % of successful PWWS attacks (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

).
 
\begin_inset CommandInset label
LatexCommand label
name "tab:text_attack_evaluation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="1">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top" width="8cm">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Text perturbed with DWB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
g
\series bold
\bar under
n
\series default
\bar default
reat historical movie, will not allow a viewer to leave once you begin to
 watch.
 View is presented differently than displayed by most school books on this
 s
\series bold
\bar under
S
\series default
\bar default
ubject [...]
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Text perturbed with PWWS
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
the acting , costumes , music , cinematography and sound are all 
\strikeout on
astounding
\series bold
\bar under
\strikeout default
dumbfounding
\series default
\bar default
 given the production 's austere locales .
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Examples of successful DWB and PWWS perturbations on a vanilla Bert model
 fine tuned over the IMDB dataset.
 The original input strings were perturbed such that inserted tokens are
 marked in underscored boldface and removed tokens in strikethrough.
 Both examples were classified correctly as 'Positive sentiment' before
 the attack and 'Negative sentiment' afterwards.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:text_attack_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:Discussion"

\end_inset


\end_layout

\begin_layout Standard
Our study strengthens the argument for using the Information Bottleneck
 combined with variational approximations to obtain robust models that can
 withstand adversarial attacks.
 By deriving a tighter bound on the IB functional, we demonstrate it's utility
 as the Variational Upper Bound (VUB) objective for neural networks.
 We demonstrate that VUB outperforms the Variational Information Bottleneck
 (VIB) in terms of test accuracy while providing similar or superior robustness
 to adversarial attacks in challenging classification tasks of different
 modalities.
\end_layout

\begin_layout Standard
Comparing VIB and VUB we observe that both methods promote a disentangled
 latent space by using a stochastic factorized prior, as suggested by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Chen2018}
\end_layout

\end_inset

.
 In addition, both methods utilize KL regularization, enforcing clustering
 around a 
\begin_inset Formula $0$
\end_inset

 mean which might increase latent smoothness.
 These traits can make it difficult for minor perturbations to significantly
 alter latent semantics, making the models more robust to attacks.
 In the case of VUB, the enhanced results induced by classifier regularization
 not only reinforce previous studies on the ELBO function, which suggest
 that overly powerful decoders diminish the quality learned representations
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018}
\end_layout

\end_inset

, but also align with the confidence penalty proposed by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Pereyra2017}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
We also observed that in many cases VIB achieves lower validation set Cross-Entr
opy while VUB achieves significantly higher test set accuracy.
 We attribute this gap to the VUB models becoming more calibrated, and we
 suggest that practitioners also monitor validation set accuracy and rate
 distortion ratio and during training.
 These metrics may be more informative indicators of model performance than
 validation set Cross-Entropy alone, as validation Cross-Entropy could increase
 as models become more calibrated.
 
\end_layout

\begin_layout Standard
We made another interesting observation during our study regarding information
 plane behavior throughout the training process.
 While previous research has documented the occurrence of error minimization
 and representation compression phases, our work revealed that these phases
 can occur in cycles throughout training.
 This finding is particularly noteworthy because previous studies observed
 this phenomenon in simple toy problems, whereas our research demonstrated
 it in a complex task with an unknown distribution and high dimensionality.
 This suggests that the behavior of the information plane is not limited
 to simplified scenarios but is a characteristic of the learning process
 in more challenging tasks as well.
 
\end_layout

\begin_layout Standard
In conclusion, while the IB and its variational approximations do not provide
 a complete theoretical framework for DNN data modeling and regularization,
 they offer a strong, measurable, and theoretically-grounded approach.
 VUB is presented as a tractable and tighter upper bound of the IB functional
 that can be easily adapted to any classifier DNN, including transformer
 based text classifiers, to significantly increase robustness to various
 adversarial attacks while inflicting minimal decrease in performance.
 This study opens many opportunities for further research.
 Besides further improvements to the upper bound, it is intriguing to use
 VUB in self-supervised learning and in generative tasks.
 Other possible directions, including measuring model calibration as proposed
 by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Achille2018}
\end_layout

\end_inset

 are left for future work.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/nirweingarten/Desktop/university/idc/thesis/icml/media/two_images/untargeted_two_images.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Successful untargeted FGS attack examples.
 Images are perturbations of previously successfully classified instances
 from the ImageNet validation set.
 Perturbation magnitude is determined by the parameter 
\begin_inset Formula $\epsilon$
\end_inset

 shown on the left, the higher the more perturbed.
 Notice the deterioration of image quality as 
\begin_inset Formula $\epsilon$
\end_inset

 increases.
 Original and wrongly assigned labels are listed at the top of each image.
\begin_inset CommandInset label
LatexCommand label
name "fig:untargeted_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Section*
Impact
\end_layout

\begin_layout Standard
This paper presents work whose goal is to advance the field of Machine Learning.
 There are many potential societal consequences of our work, none which
 we feel must be specifically highlighted here.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bibliography{draft.bib}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
