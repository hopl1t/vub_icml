#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage{icml2023}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
%\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Tighter Bounds on the Information Bottleneck with Application to Deep Learning}

\bibliographystyle{icml2023}
\end_preamble
\options letterpaper
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date true
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
twocolumn[ 
\backslash
icmltitle{Tighter Bounds on the Information Bottleneck with Application
 to Deep Learning}
\end_layout

\begin_layout Plain Layout


\backslash
icmlsetsymbol{equal}{*}
\end_layout

\begin_layout Plain Layout


\backslash
begin{icmlauthorlist} 
\backslash
icmlauthor{Nir Weingarten}{runi} 
\backslash
icmlauthor{Zohar Yakhini}{runi} 
\backslash
icmlauthor{Moshe Butman}{runi} 
\backslash
icmlauthor{Ran Gilad-Bachrach}{tau} 
\backslash
end{icmlauthorlist}
\end_layout

\begin_layout Plain Layout


\backslash
icmlaffiliation{runi}{Efi Arzi School of Computer Science, Reichman University,
 Herzliya, Israel} 
\backslash
icmlaffiliation{tau}{Department of Biomedical Engineering, Tel-Aviv University,
 Tel-Aviv, Israel}
\end_layout

\begin_layout Plain Layout


\backslash
icmlcorrespondingauthor{Nir Weingarten}{nir.weingarten@runi.ac.il}
\end_layout

\begin_layout Plain Layout

% You may provide any keywords that you % find helpful for describing your
 paper; these are used to populate % the "keywords" metadata in the PDF
 but will not be shown in the document 
\backslash
icmlkeywords{Information Bottleneck, Information Theory, Representation
 Learning, Regularization, Robustness, Adversarial Attacks}
\end_layout

\begin_layout Plain Layout


\backslash
vskip 0.3in ]
\end_layout

\begin_layout Plain Layout

% This command actually creates the footnote in the first column
\end_layout

\begin_layout Plain Layout


\backslash
printAffiliationsAndNotice{}
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
Deep Neural Nets (DNNs) learn latent representations induced by their downstream
 task, objective function, and other parameters.
 The quality of the learned representations impacts the DNN's generalization
 ability and the coherence of the emerging latent space.
 The Information Bottleneck (IB) provides a hypothetically optimal framework
 for data modeling, yet it is often intractable.
 Recent efforts combined DNNs with the IB by applying VAE-inspired variational
 methods to approximate bounds on mutual information, resulting in improved
 robustness to adversarial attacks.
 This work introduces a new and tighter variational bound for the IB, improving
 performance of previous IB-inspired DNNs.
 These advancements strengthen the case for the IB and its variational approxima
tions as a data modeling framework, and provide a simple method to significantly
 enhance the adversarial robustness of classifier DNNs.
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
In recent years, Deep Neural Networks (DNNs) have gained prominence in various
 learning tasks, revolutionizing many computational fields with their ability
 to approximate complex functions.
 Despite the great achievements, it is still postulated that the current
 networks are prone to overfit the training data 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Ying2019}
\end_layout

\end_inset

, may be considerably uncalibrated 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Guo2017}
\end_layout

\end_inset

 and are susceptible to adversarial attacks 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Goodfellow2015}
\end_layout

\end_inset

.
 A question emerges regarding the extraction of an optimal representation
 for all data points from a restricted set of training examples.
 Classic information theory provides tools to optimize compression and transmiss
ion of data, but it does not provide methods to gauge the relevance of a
 compressed signal to its downstream task.
 Methods such as rate-distortion 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Blahut1972}
\end_layout

\end_inset

 regard all information as equal, not taking into account which information
 is more relevant without constructing complex distortion functions.
 The Information Bottleneck (IB) 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

 resolves this limitation by defining mutual information between the learned
 representation and the downstream task as a universal distortion function.
 Under this definition, an optimal rate-disotrion ratio can be implicitly
 computed for a Lagrange multiplier 
\begin_inset Formula $\beta$
\end_inset

, controlling the tradeoff between the desired rate and distortion.
 However, optimizing over the IB requires mutual information computations,
 which are tractable in discrete settings and for some specific continuous
 distributions.
 Adopting the IB framework for DNNs requires computing mutual information
 for unknown distributions and has no analytic solution.
 However, recent work approximated tractable upper bounds for the IB functional
 in DNN settings using variational approximations.
 Variational Auto Encoders (VAEs) 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2014}
\end_layout

\end_inset

 use stochastic DNNs to approximate intractable distributions, as elaborated
 in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:variational_approximations"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Similarly to VAEs, 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

 proposed using stochastic DNNs as variational approximations of latent
 models, thus making possible the computations of upper bounds for mutual
 information between the DNN's input, output and latent representation.
 A proposed DNN optimization method called Deep Variational Information
 Bottleneck (VIB) derives an upper bound for the IB objective and minimizes
 its approximation by fitting some training dataset.
 Optimizing classifier DNNs with the VIB objective results in a slight decrease
 in test set accuracy compared to deterministic DNNs, but yields a significant
 increase in robustness to adversarial attacks.
\end_layout

\begin_layout Standard
In this study, we adopt the same information theoretic and variational approach
 proposed in VIB.
 The work begins by deriving a new upper bound for the IB functional.
 We then employ a tractable variational approximation for this bound, named
 VUB - 'Variational Upper Bound' and show that it is a tighter bound on
 the IB objective than VIB.
 We proceed to show empirical evidence that VUB substantially increases
 test set accuracy over VIB while providing similar or superior robustness
 to adversarial attacks across several challenging tasks and different modalitie
s.
 Finally, we discuss these effects in the context of previous work on the
 IB and on DNN regularization.
 The conclusion drawn is that while increasing mutual information between
 encoding and output does not necessarily improve classification, and while
 increasing encoding compression does not always enhance regularization
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Amjad2020}
\end_layout

\end_inset

, the application of IB approximations as objectives to DNNs empirically
 improves regularization, suggesting better data modeling.
 This notion contributes for the adaptation of the IB, and its variational
 approximations, as an objective for learning tasks and as a theoretic framework
 to gauge and explain data modeling.
\end_layout

\begin_layout Standard
In addition, we demonstrate that VUB can be easily adapted to any classifier
 DNN, including transformer based NLP classifiers, to substantially increase
 robustness to adversarial attacks while only slightly decreasing, or in
 some cases even increasing, test set accuracy.
\end_layout

\begin_layout Subsection
Preliminaries
\end_layout

\begin_layout Standard
The following literature review and derivations refer to information theory
 and variational approximations.
 A preliminary mutual ground and notation is provided.
\end_layout

\begin_layout Standard
We denote random variables (RVs) with upper cased letters 
\begin_inset Formula $X,Y$
\end_inset

, and their realizations in lower case 
\begin_inset Formula $x,y$
\end_inset

.
 Denote discrete Probability Mass Functions (PMFs) with an upper case 
\begin_inset Formula $P(x)$
\end_inset

 and continuous Probability Density Functions (PDFs) with a lower case 
\begin_inset Formula $p(x)$
\end_inset

.
 Hat notation denotes empirical measurements.
 
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $X,Y$
\end_inset

 be two observed random variables with unknown distributions 
\begin_inset Formula $p^{*}(x),p^{*}(y)$
\end_inset

 that we aim to model.
 Assume 
\begin_inset Formula $X,Y$
\end_inset

 are governed by some unknown underlying process with a joint probability
 distribution 
\begin_inset Formula $p^{*}(x,y)$
\end_inset

.
 We can attempt to approximate these distributions using a model 
\begin_inset Formula $p_{\theta}$
\end_inset

 with parameters 
\begin_inset Formula $\theta$
\end_inset

 such that for generative tasks 
\begin_inset Formula $p_{\theta}(x)\approx p^{*}(x)$
\end_inset

 and for discriminative tasks 
\begin_inset Formula $p_{\theta}(y|x)\approx p^{*}(y|x)$
\end_inset

, using a dataset 
\begin_inset Formula $\mathcal{S}=\left\{ (x_{1},y_{1}),...,(x_{N},y_{N})\right\} $
\end_inset

 to fit our model.
 One can also assume the existence of an additional unobserved RV 
\begin_inset Formula $Z\sim p^{*}(z)$
\end_inset

 that influences or generates the observed RVs 
\begin_inset Formula $X,Y$
\end_inset

.
 Since 
\begin_inset Formula $Z$
\end_inset

 is unobserved it is absent from the dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

 and so cannot be modeled directly.
 Denote 
\begin_inset Formula $\int p_{\theta}(x|z)p_{\theta}(z)dz$
\end_inset

 the marginal, 
\begin_inset Formula $p_{\theta}(z)$
\end_inset

 the prior as it is not conditioned over any other RV, and 
\begin_inset Formula $p_{\theta}(z|x)$
\end_inset

 the posterior following Bayes' rule.
 
\end_layout

\begin_layout Standard
When modeling an unobserved variable of an unknown distribution we encounter
 a problem as the marginal 
\begin_inset Formula $p_{\theta}(x)=\int p_{\theta}(x,z)dz$
\end_inset

 doesn't have an analytic solution.
 This intractability can be overcome by choosing some tractable parametric
 variational distribution 
\begin_inset Formula $q_{\phi}(z|x)$
\end_inset

 to approximate the posterior 
\begin_inset Formula $p_{\theta}(z|x)$
\end_inset

 such that 
\begin_inset Formula $q_{\phi}(z|x)\approx p_{\theta}(z|x)$
\end_inset

, and estimate 
\begin_inset Formula $p_{\theta}(x,z)$
\end_inset

 or 
\begin_inset Formula $p_{\theta}(x,z|y)$
\end_inset

 by fitting the dataset 
\begin_inset Formula $\mathcal{S}$
\end_inset

 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Kingma2019}
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
In this work information theoretic functions share the same notation for
 discrete and continuous settings.
 For brevity, we will only present the continuous form:
\end_layout

\begin_layout Standard
\align left
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="2">
<features booktabs="true" tabularvalignment="middle">
<column alignment="left" valignment="middle" width="2.5cm">
<column alignment="left" valignment="middle" width="5cm">
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
\begin_inset Formula $H_{p}(X)=-\int p(x)log\left(p(x)\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cross
\end_layout

\begin_layout Plain Layout
Entropy
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $CE(p,q)=-\int p(x)log\left(q(x)\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
KL
\end_layout

\begin_layout Plain Layout
Divergence
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $D_{KL}\left(p\big|\big|q\right)=\int p(x)log\left(\frac{p(x)}{q(x)}\right)dx$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row interlinespace="0.2cm">
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Mutual Information
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $I(X;Y)=\int\int p(x,y)log\left(\frac{p(x,y)}{p(x)p(y)}\right)dxdy$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Section
Related work
\end_layout

\begin_layout Subsection
IB and its analytic solutions
\end_layout

\begin_layout Standard
Classic information theory offers rate-distortion 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Blahut1972}
\end_layout

\end_inset

 to mitigate signal loss during compression.
 Rate being the signal's compression measured by mutual information between
 input and output signals, and distortion a chosen task-specific function.
 The Information Bottleneck (IB) method 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Tishby1999}
\end_layout

\end_inset

 extends rate-distortion by replacing the tailored distortion functions
 with mutual information between the learned representation and the downstream
 task.
 Denote 
\begin_inset Formula $X$
\end_inset

 the source signal, 
\begin_inset Formula $Z$
\end_inset

 its encoding and 
\begin_inset Formula $Y$
\end_inset

 the target signal for some specific task.
 Assuming a latent variable model that follows the Markov chain 
\begin_inset Formula $Z\leftrightarrow X\leftrightarrow Y$
\end_inset

, we define some positive minimal threshold 
\begin_inset Formula $D$
\end_inset

 for the desired distortion.
 We seek an optimal encoding 
\begin_inset Formula $Z:\underset{P(Z|X)}{min}I(X;Z)$
\end_inset

 subject to 
\begin_inset Formula $I(Z;Y)\ge D$
\end_inset

.
 This constrained problem can be implicitly optimized by minimizing the
 functional 
\begin_inset Formula $L_{P(Z|X)}=I(Z;X)-\beta I(Z;Y)$
\end_inset

, the first term being rate and the second distortion modulated by the Lagrange
 multiplier 
\begin_inset Formula $\beta$
\end_inset

.
 The optimal solution is a function of 
\begin_inset Formula $\beta$
\end_inset

 and was named 'the information curve' as illustrated in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{info_plane}
\end_layout

\end_inset

.
 The IB can be interpreted as a method to learn a representation that holds
 just enough information to satisfy a desired task, while discarding all
 other available information, presumably providing a model with the least
 possible complexity.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../aaai/media/info_plane.png
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
The information plane and curve: rate-distortion ratio over 
\begin_inset Formula $\beta$
\end_inset

.
 At 
\begin_inset Formula $\beta=0$
\end_inset

 the representation is compressed but uninformative (maximal compression),
 at 
\begin_inset Formula $\beta\rightarrow\infty$
\end_inset

 the representation is informative but potentially overfitted (maximal informati
on).
 Adapted from 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Slonim2002}
\end_layout

\end_inset

.
\begin_inset CommandInset label
LatexCommand label
name "info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The IB functional is only tractable when mutual information can be computed
 and was originally demonstrated for soft clustering tasks over a discrete
 and known distribution 
\begin_inset Formula $P^{*}(X,Y)$
\end_inset

.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Chechik2003}
\end_layout

\end_inset

 extended the IB for gaussian distributions and 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Painsky2017}
\end_layout

\end_inset

 offered a limited linear approximation of the IB for any distribution.
\end_layout

\begin_layout Subsection
IB and deep learning
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Tishby2015}
\end_layout

\end_inset

 proposed an IB interpretation of DNNs, regarding them as Markov cascades
 of intermediate representations between hidden layers.
 Under this framework, comparing the optimal and the achieved rate-distortion
 ratios between DNN layers will indicate if a model is too complex or too
 simple for a given task and training set.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

 visualized and analyzed the information plane behavior of DNNs over a toy
 problem with a known joint distribution.
 Mutual information of the different layers was estimated and used to analyze
 the training process.
 The learning process over Stochastic Gradient Descent (SGD) exhibited two
 separate and sequential behaviors: A short Empirical Error Minimization
 phase (ERM) characterized by a rapid decrease in distortion, followed by
 a long compression phase with an increase in rate until convergence to
 an optimal IB limit as demonstrated in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{shwartz_ziv_info_plane}
\end_layout

\end_inset

.
 Similar yet repetitive behavior was observed in the current study, as elaborate
d in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{subsec:Evaluation-and-analysis}
\end_layout

\end_inset

.
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\noindent
\align center
\begin_inset Graphics
	filename ../../aaai/media/black_box_z.png
	scale 36

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Information plane scatters of different DNN layers (colors) in 50 randomized
 networks.
 From 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.
 Left are initial weights, Right are at 400 epochs.
 Our study reproduced similar yet repetitive behavior on complicated high
 dimensional tasks, as elaborated in Section
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{subsec:Evaluation-and-analysis}
\end_layout

\end_inset

 and in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{estimated_info_plane}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "shwartz_ziv_info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Amjad2020}
\end_layout

\end_inset

 pointed out three flaws in the usage of the IB functional as an objective
 for deterministic DNN classifiers: (1) When data 
\begin_inset Formula $X$
\end_inset

 is absolutely continuous the mutual information term 
\begin_inset Formula $I(X;Z)$
\end_inset

 is infinite; (2) When data 
\begin_inset Formula $X$
\end_inset

 is discrete the IB functional is a piecewise constant function of the parameter
s, making it's SGD optimization difficult or impossible; (3) Equivalent
 representations might yield the same IB loss while one achieved better
 classification rate than the other.
 These discrepancies were attributed to mutual information's invariance
 to invertible transformations and to the absence of a decision function
 in the objective.
\end_layout

\begin_layout Subsection
Variational approximations to the IB objective
\begin_inset CommandInset label
LatexCommand label
name "subsec:variational_approximations"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Kingma2014}
\end_layout

\end_inset

 introduced the Variational Auto Encoder (VAE) - a stochastic generative
 DNN.
 An unobserved RV 
\begin_inset Formula $Z$
\end_inset

 is assumed to generate evidence 
\begin_inset Formula $X$
\end_inset

 and the true probability 
\begin_inset Formula $p^{*}(x)$
\end_inset

 can be modeled using a parametric model over the marginal 
\begin_inset Formula $p_{\theta}(x)=\int p_{\theta}(x|z)p_{\theta}(z)dz$
\end_inset

.
 However, since the marginal is intractable a variational approximation
 
\begin_inset Formula $q_{\phi}(z|x)\approx p_{\theta}(z|x)$
\end_inset

 is proposed instead.
 The log probability 
\begin_inset Formula $log\left(p_{\theta}(x)\right)$
\end_inset

 is then developed in to the tractable VAE loss comprised of the Evidence
 Lower Bound (ELBO) and KL regularization terms: 
\begin_inset Formula $\underset{\text{\emph{\text{\emph{ELBO}}}}}{\mathcal{L}}=\mathbb{E}_{q_{\phi}(z|x)}\left[log\left(p_{\theta}(x|z)\right)\right]-D_{KL}\left(q_{\phi}(z|x)\big|\big|p_{\theta}(z)\right)$
\end_inset

.
 
\begin_inset Formula $q_{\phi}(z|x)$
\end_inset

 is modeled using a stochastic neural encoder having it's final activation
 used as parameters for the assumed variational distribution (typically
 a spherical gaussian with parameters 
\begin_inset Formula $\mu,\Sigma$
\end_inset

).
 Each forward pass emulates a stochastic realization 
\begin_inset Formula $z\in Z$
\end_inset

 from these parameters by using the 'reparameterization trick': 
\begin_inset Formula $z=\mu+\epsilon\cdot\Sigma$
\end_inset

 for some unparameterized scalar 
\begin_inset Formula $\epsilon\sim N(0,1)$
\end_inset

 such that a backwards pass is possible.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Higgins2017}
\end_layout

\end_inset

 later proposed the 
\begin_inset Formula $\beta$
\end_inset

-autoencoder, introducing a hyper parameter 
\begin_inset Formula $\beta$
\end_inset

 over the KL term to control the regularization-reconstruction tradeoff.
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

 found that as the ELBO loss in VAEs depends solely on image reconstruction
 it does not necessarily induce a better quality modeling of the marginal
 
\begin_inset Formula $p_{\theta}(z)$
\end_inset

, hence not necessarily a better representation learned.
 This gap is attributed to powerful decoders being overfitted, as will be
 further discussed in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Discussion"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

 introduced the Variational Information Bottleneck (VIB) as a variational
 approximation for an upper bound to the IB objective for classifier DNN
 optimization.
 Bounds for 
\begin_inset Formula $I(Z,Y)$
\end_inset

 and 
\begin_inset Formula $I(X,Z)$
\end_inset

 are derived from the non negativity of KL divergence and are used to form
 an upper bound for the IB functional.
 This upper bound is approximated using variational approximations for 
\begin_inset Formula $p^{*}(y|z),\,p^{*}(z)$
\end_inset

 as done in VAEs.
 This approximation of an upper bound for the IB objective is empirically
 estimated as cross entropy and a beta scaled KL regularization term as
 in 
\begin_inset Formula $\beta$
\end_inset

-autoencoders, and is optimized over the training data using Monte Carlo
 sampling and the reparameterization trick.
 VIB was evaluated over image classification tasks and, while causing a
 slight reduction in test set accuracy, generated substantial improvements
 in robustness to adversarial attacks.
\end_layout

\begin_layout Standard
Additional noteworthy contributions to this field have been made in recent
 years by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Achille2018,Wieczorek2019,Fischer2020}
\end_layout

\end_inset

 and others.
 However, a detailed review of these works is beyond the scope of this paper.
\end_layout

\begin_layout Subsection
Non IB information theoretic regularization
\end_layout

\begin_layout Standard
Label smoothing 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Szegedy2016}
\end_layout

\end_inset

 and entropy regularization 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Pereyra2017}
\end_layout

\end_inset

 both regularize classifier DNNs by increasing the entropy of their output.
 This is done either directly by inserting a scaled conditional entropy
 term to the loss function, 
\begin_inset Formula $-\gamma\cdot H\left(p_{\theta}(y|x)\right)$
\end_inset

, or by smoothing the training data labels.
 Applying both methods was demonstrated to improve test accuracy and model
 calibration on various challenging classification tasks.
 In the current work a similar conditional entropy term emerges from the
 derivation of the new upper bound for the IB objective, as shown in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:vub"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Section
From VIB to VUB
\begin_inset CommandInset label
LatexCommand label
name "sec:vub"

\end_inset


\end_layout

\begin_layout Standard
The VIB loss consists of a cross entropy term and a KL regularization term,
 as in VAE loss.
 The KL term is derived from a bound on the IB rate term 
\begin_inset Formula $I(X;Z)$
\end_inset

, while the cross entropy term from a bound on the IB distortion term 
\begin_inset Formula $I(Z;Y)=H(Y)-H(Y|Z)$
\end_inset

.
 When deriving the latter the entropy term 
\begin_inset Formula $H(Y)$
\end_inset

 is ignored as it is constant and does not effect optimization.
 We note that since 
\begin_inset Formula $Y$
\end_inset

 is unknown any optimization over 
\begin_inset Formula $Z$
\end_inset

, including cross entropy, depends on our decoder model of 
\begin_inset Formula $Y$
\end_inset

.
 Following this logic, instead of omitting 
\begin_inset Formula $H(Y)$
\end_inset

 we replace it with a variational approximation of the decoder entropy,
 which provides a lower bound.
\end_layout

\begin_layout Subsection
IB upper bound
\end_layout

\begin_layout Standard
We begin by establishing a new upper bound for the IB functional by bounding
 the mutual information terms, using the same method as in VIB.
\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula $I(Z;X)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;X)= & \int\int p^{*}(x,z)log\left(p^{*}(z|x)\right)dxdz\nonumber \\
- & \int p^{*}(z)log\left(p^{*}(z)\right)dz\label{eq:i_z_x1}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
For any probability distribution 
\begin_inset Formula $r$
\end_inset

 we have that 
\begin_inset Formula $D_{KL}\left(p^{*}(z)\big|\big|r(z)\right)\ge0$
\end_inset

, it follows that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\int p^{*}(z)log\left(p^{*}(z)\right)dz\ge\int p^{*}(z)log\left(r(z)\right)dz\label{eq:d_kl}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And so by Equation
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:d_kl}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
I(Z;X)\le\int\int p^{*}(x)p^{*}(z|x)log\left(\frac{p^{*}(z|x)}{r(z)}\right)dxdz\label{eq:i_z_x2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Consider 
\begin_inset Formula $I(Z;Y)$
\end_inset

:
\end_layout

\begin_layout Standard
For any probability distribution 
\begin_inset Formula $c$
\end_inset

 we have that 
\begin_inset Formula $D_{KL}\left(p^{*}(y|z)\big|\big|c(y|z)\right)\ge0$
\end_inset

, it follows that:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\int p^{*}(y|z)log\left(p^{*}(y|z)\right)dy\ge\int p^{*}(y|z)log\left(c(y|z)\right)dy\label{eq:d_kl2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And so by Equation
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:d_kl2}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;Y)= & \int\int p^{*}(y,z)log\left(\frac{p^{*}(y,z)}{p^{*}(y)p^{*}(z)}\right)dydz\nonumber \\
\ge & \int\int p^{*}(y|z)p^{*}(z)log\left(\frac{c(y|z)}{p^{*}(y)}\right)dydz\nonumber \\
= & \int\int p^{*}(y,z)log\left(c(y|z)\right)dydz+H_{p^{*}}(Y)\label{eq:i_z_y}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
We now diverge from the original VIB derivation by replacing 
\begin_inset Formula $H_{p^{*}}(Y)$
\end_inset

 with 
\begin_inset Formula $H_{c}(Y|Z)$
\end_inset

 instead of omitting it.
 In addition, we limit the new term to make sure that the inequality 
\begin_inset Formula $H(Y|Z)\le H(Y)$
\end_inset

 holds when computing entropy over the different distributions 
\begin_inset Formula $p^{*}$
\end_inset

 and 
\begin_inset Formula $c$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
I(Z;Y)\ge & \int\int p^{*}(y,z)log\left(c(y|z)\right)dydz\nonumber \\
+ & min\left\{ H_{p^{*}}(Y),H_{c}(Y|Z)\right\} \label{eq:i_z_y2}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
We further develop this term using the IB Markov chain 
\begin_inset Formula $Z\leftrightarrow X\leftrightarrow Y$
\end_inset

 and total probability:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
 & I(Z;Y)\ge\nonumber \\
 & \int\int\int p^{*}(x)p^{*}(y|x)p^{*}(z|x)log\left(c(y|z)\right)dxdydz\nonumber \\
- & min\left\{ H_{p^{*}}(Y),-\int\int c(y,z)log\left(c(y|z)\right)dydz\right\} \label{eq:i_z_y3}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
Finally, we define a new upper bound for the IB functional named 
\begin_inset Formula $L_{UB}$
\end_inset

 by joining the bound on rate in Equation 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:i_z_x2}
\end_layout

\end_inset

 with the bound on distortion in Equation 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{eq:i_z_y3}
\end_layout

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
 & L_{UB}\equiv\nonumber \\
 & \int\int p^{*}(x)p^{*}(z|x)log\left(\frac{p^{*}(z|x)}{r(z)}\right)dxdz\nonumber \\
- & \int\int\int p^{*}(x)p^{*}(y|x)p^{*}(z|x)log\left(c(y|z)\right)dxdydz\nonumber \\
+ & min\left\{ H_{p^{*}}(Y),-\int\int c(y,z)log\left(c(y|z)\right)dydz\right\} \label{eq:l_ub}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
It is easy to verify that the bound holds for all 
\begin_inset Formula $\beta\ge0$
\end_inset

 such that 
\begin_inset Formula $L_{IB}=\beta\cdot I\left(Z;X\right)-I\left(Z;Y\right)$
\end_inset

.
\end_layout

\begin_layout Subsection
Variational approximation
\end_layout

\begin_layout Standard
Following the same variational approach as in VIB, we define 
\begin_inset Formula $L_{VUB}$
\end_inset

 as a new tractable upper bound for the IB functional.
 Let 
\begin_inset Formula $p^{*}(x,y,z)$
\end_inset

 be the unknown joint distribution, 
\begin_inset Formula $e(z|x)$
\end_inset

 a variational encoder approximating 
\begin_inset Formula $p^{*}(z|x)$
\end_inset

 and 
\begin_inset Formula $c(y|z)$
\end_inset

 a variational classifier approximating 
\begin_inset Formula $p^{*}(y|z)$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
 & L_{VUB}\equiv\nonumber \\
 & \beta\int\int p^{*}(x)e(z|x)log\left(\frac{e(z|x)}{r(z)}\right)dxdz\nonumber \\
- & \int\int\int p^{*}(x)p^{*}(y|x)e(z|x)log\left(c(y|z)\right)dxdydz\label{eq:l_vub}\\
- & min\Bigg\{ H_{p^{*}}(Y),\nonumber \\
 & -\int\int\int p^{*}(x)e(z|x)c(y|z)log\left(c(y|z)\right)dxdydz\Bigg\}\nonumber \\
\ge & L_{IB}\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Subsection
Empirical estimation
\end_layout

\begin_layout Standard
We proceed to model VUB using DNNs and optimize it using Monte Carlo sampling
 over some training dataset.
 Let 
\begin_inset Formula $e_{\phi}$
\end_inset

 be a stochastic DNN encoder with parameters 
\begin_inset Formula $\phi$
\end_inset

 applying the reparameterization trick such that 
\begin_inset Formula $e_{\phi}(x)\sim N(\mu,\Sigma)$
\end_inset

 and
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 let 
\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit

\begin_inset Formula $C_{\lambda}$
\end_inset

 be a discrete classifier DNN parameterized by 
\begin_inset Formula $\lambda$
\end_inset

 such that 
\begin_inset Formula $C_{\lambda}(\hat{z})\sim Categorical$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align}
\hat{L}_{VUB} & \equiv\nonumber \\
 & \frac{1}{N}\sum_{n=1}^{N}\Bigg[\beta\cdot D_{KL}\left(e_{\phi}(x_{n})\big|\big|r(z)\right)\label{eq:l_vub_empirical}\\
- & P^{*}(y_{n})\cdot log\left(C_{\lambda}\left(e_{\phi}(x_{n})\right)\right)\nonumber \\
- & min\left\{ H(\hat{Y}),H\left(C_{\lambda}\left(e_{\phi}(x_{n})\right)\right)\right\} \Bigg]\nonumber 
\end{align}

\end_inset


\end_layout

\begin_layout Standard
As in VIB and VAE, 
\begin_inset Formula $e_{\phi}(x)$
\end_inset

 and 
\begin_inset Formula $r(z)$
\end_inset

 are computed as spherical gaussians.
 
\begin_inset Formula $e_{\phi}(x)$
\end_inset

 by using the first half of the encoder's output entries as 
\begin_inset Formula $\mu$
\end_inset

 and the second as the diagonal 
\begin_inset Formula $\Sigma$
\end_inset

, and 
\begin_inset Formula $r(z)$
\end_inset

 by a standard normal gaussian.
\end_layout

\begin_layout Subsection
Interpretation
\end_layout

\begin_layout Standard
Similarly to the confidence penalty suggested by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Pereyra2017}
\end_layout

\end_inset

, the new derivation adds classifier regularization to the VIB objective.
 Regularizing the classifier might prevent it from overfitting, and is a
 possible remedy to the discrepancies in the ELBO loss observed by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2018}
\end_layout

\end_inset

, as elaborated in Section
\begin_inset space ~
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Discussion"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
\end_layout

\begin_layout Standard
In terms of tightness we have that VUB is a tighter theoretical bound on
 the IB objective than VIB for any 
\begin_inset Formula $Y$
\end_inset

 such that 
\begin_inset Formula $H(Y)>0$
\end_inset

, and a tighter empirical bound for all 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Section
Experiments
\begin_inset CommandInset label
LatexCommand label
name "sec:Experiments"

\end_inset


\end_layout

\begin_layout Standard
We follow the experimental setup proposed by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Alemi2017}
\end_layout

\end_inset

, extending it to NLP tasks as well.
 Image classification models were trained on the ImageNet 2012 dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{deng2009imagenet}
\end_layout

\end_inset

 and text classification over
\series bold
 
\series default
the IMDB
\series bold
 
\series default
sentiment analysis dataset 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{maas2011imdb}
\end_layout

\end_inset

.
 For each dataset, a competitive pre-trained model (Vanilla model) was evaluated
 and then used to encode embeddings.
 These embeddings were then used as a dataset for a new stochastic classifier
 net with either a VIB or a VUB loss function.
 Stochastic classifiers consisted of two ReLU activated linear layers of
 the same dimensions as the pre-trained model's logits (2048 for image and
 768 for text classification), followed by reparameterization and a final
 softmax activated FC layer.
 Learning rate was 
\begin_inset Formula $10^{-4}$
\end_inset

 and decaying exponentially with a factor of 0.97 every two epochs.
 Batch sizes were 32 for ImageNet and 16 for IMDB.
 We used a single forward pass per sample for inference.
 Each model was trained and evaluated 5 times per 
\begin_inset Formula $\beta$
\end_inset

 value with consistent performance.
 Beta values of 
\begin_inset Formula $\beta=10^{-i}$
\end_inset

 for 
\begin_inset Formula $i\in\{1,2,3\}$
\end_inset

 were tested since previous studies indicated this is the best range for
 VIB 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2017,Alemi2018}
\end_layout

\end_inset

.
 Each model was evaluated using test set accuracy and robustness to various
 adversarial attacks over the test set.
 For image classification we employed the untargeted Fast Gradient Sign
 (FGS) attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Goodfellow2015}
\end_layout

\end_inset

 as well as the targeted CW 
\begin_inset Formula $L_{2}$
\end_inset

 optimization attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Carlini2017}, 
\backslash
cite{Kaiwen2018}
\end_layout

\end_inset

.
 For text classification we used the untargeted Deep Word Bug attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{gao2018deepwordbug}
\end_layout

\end_inset

 as well as the untargeted PWWS attack 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{ren2019pwws}, 
\backslash
cite{Morris2020}
\end_layout

\end_inset

.
 All models were trained using an Nvidia RTX3080 GPU.
 Code to reconstruct the experiments is provided in the included code &
 data supplementary material.
\end_layout

\begin_layout Subsection
Image classification
\end_layout

\begin_layout Standard
A pre-trained inceptionV3 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Szegedy2016}
\end_layout

\end_inset

 base model was used and achieved a 77.21% accuracy on the ImageNet 2012
 validation set (Test set for ImageNet is unavailable).
 Note that inceptionV3 yields a slightly worse single shot accuracy than
 inceptionV2 (80.4%) when run in a single model and single crop setting,
 however we've used InceptionV3 over V2 for simplicity.
 Each model was trained for 100 epochs.
 The entire validation set was used to measure accuracy and robustness to
 FGS attacks, while only 1% of it was used for CW attacks as they are computatio
nally expensive.
\end_layout

\begin_layout Subsubsection
Evaluation and analysis
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="5">
<features booktabs="true" tabularvalignment="middle" tabularwidth="8cm">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<column alignment="center" valignment="middle" width="2.7pheight%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Val 
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\underset{\epsilon=0.1}{\text{FGS}}\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\underset{\epsilon=0.5}{\text{FGS}}\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
CW
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vanilla model
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
77.2%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
68.9%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
67.7%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
788
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VIB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
73.7%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
59.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
63.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3917
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm291$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
72.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
53.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3318
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm293$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="middle" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
72.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.01\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
58.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3318
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm293$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VUB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.03\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
66.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
2666
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm140$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
75.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.05\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
57.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.2\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
64.3%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.1\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
1564
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm218$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
74.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.09\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
57.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
64.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
3575
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm456$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
ImageNet evaluation scores for vanilla, VIB and VUB models, average over
 5 runs with standard deviation.
 First column is performance on the ImageNet validation set (higher is better
 
\begin_inset Formula $\uparrow$
\end_inset

), second and third columns are the % of successful FGS attacks at 
\begin_inset Formula $\epsilon=0.1,0.5$
\end_inset

 (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

) and the fourth column is the average 
\begin_inset Formula $L_{2}$
\end_inset

 distance for a successful Carlini Wagner 
\begin_inset Formula $L_{2}$
\end_inset

 targeted attack (higher is better 
\begin_inset Formula $\uparrow$
\end_inset

).
\begin_inset CommandInset label
LatexCommand label
name "tab:imagenet_evaluation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

Image classification evaluation results are shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:imagenet_evaluation}
\end_layout

\end_inset

, examples of successful attacks are shown in Figures
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:targeted_examples}, 
\backslash
ref{fig:untargeted_examples}
\end_layout

\end_inset

.
 The empirical results presented in Table 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:imagenet_evaluation}
\end_layout

\end_inset

 confirm that while VIB and VUB reduce performance on the validation set,
 they substantially improve robustness to adversarial attacks.
 Moreover, these results demonstrate that VUB significantly outperforms
 VIB in terms of validation set accuracy while providing competitive robustness
 to attacks.
 A comparison of the best VIB and VUB models further substantiates these
 findings, with statistical significance confirmed by a p-value of less
 than 0.05 in a Wilcoxon rank sum test.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../../aaai/media/info_plane_imdb.png
	scale 31

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Estimated information plane metrics per epoch for VUB trained on the IMDB
 dataset with 
\begin_inset Formula $\beta=0.001$
\end_inset

.
 
\begin_inset Formula $I(Z;X)$
\end_inset

 is approximated by 
\begin_inset Formula $H(R)-H(Z|X)$
\end_inset

 and 
\begin_inset Formula $\frac{1}{CE(Y;\hat{Y})}$
\end_inset

 is used as an analog for 
\begin_inset Formula $I(Z;Y)$
\end_inset

.
 The epochs have been grouped and color-coded in intervals of 30 epochs
 in the order: Orange (0-30), gray (30-60), yellow (60-90), green (90-120)
 and red (120-150).
 We notice recurring patterns of distortion reduction followed by rate increase,
 resembling the ERM and representation compression stages described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.
 
\begin_inset CommandInset label
LatexCommand label
name "estimated_info_plane"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/nirweingarten/Desktop/university/idc/thesis/icml/media/two_images/targeted_two_images.png
	scale 38

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Successful targeted CW attack examples.
 Images are perturbations of previously successfully classified instances
 from the ImageNet validation set.
 The target label is 'Soccer ball'.
 Average 
\begin_inset Formula $L_{2}$
\end_inset

 distance required for a successful attack is shown on the left.
 The higher the required 
\begin_inset Formula $L_{2}$
\end_inset

 distance the greater the visible change required to fool the model.
 Original and wrongly assigned labels are listed at the top of each image.
 Mind the difference in noticeable change as compared to the FGS perturbations
 presented in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{fig:untargeted_examples}
\end_layout

\end_inset

, and between VIB and VUB perturbations.
\begin_inset CommandInset label
LatexCommand label
name "fig:targeted_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Text classification
\end_layout

\begin_layout Standard
A fine tuned BERT uncased 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Devlin2019}
\end_layout

\end_inset

 base model was used and achieved a 93.0% accuracy on the
\series bold
 
\series default
IMDB sentiment analysis test set.
 Each model was trained for 150 epochs.
 The entire test set was used to measure accuracy, while only the first
 200 entries in the test set were used for adversarial attacks as they are
 computationally expensive.
\end_layout

\begin_layout Subsubsection
Evaluation and analysis
\begin_inset CommandInset label
LatexCommand label
name "subsec:Evaluation-and-analysis"

\end_inset


\end_layout

\begin_layout Standard
Text classification evaluation results are shown in Table
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_evaluation}
\end_layout

\end_inset

, examples of successful attacks are shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{tab:text_attack_examples}
\end_layout

\end_inset

.
 In this modality VUB significantly outperforms VIB in both test set accuracy
 and robustness to both attacks.
 Moreover, VUB also outperomed the original model in terms of test set accuracy.
 A comparison of the best VIB and VUB models further substantiates these
 findings, with statistical significance confirmed by a p-value of less
 than 0.05 in a Wilcoxon rank sum test.
\end_layout

\begin_layout Standard
In addition to the above evaluation metrics, we also measured approximated
 rate and distortion throughout training and plotted them on the information
 curve as shown in Figure
\begin_inset space ~
\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
ref{estimated_info_plane}
\end_layout

\end_inset

.
 We notice recurring patterns of distortion reduction followed by rate increase,
 resembling the ERM and representation compression stages described by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{ShwartzZiv2017}
\end_layout

\end_inset

.

\series bold
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="11" columns="4">
<features booktabs="true" tabularvalignment="middle" tabularwidth="8cm">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<column alignment="center" valignment="top" width="4pheight%">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Test
\begin_inset Formula $\uparrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
DWB
\begin_inset Formula $\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
PWWS
\begin_inset Formula $\downarrow$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Vanilla model
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
-
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
93.0%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
54.3%
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
100%
\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VIB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
91.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
35.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.4\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
41.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm6.6\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
90.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
41.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.8\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
62.9%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm14.3\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
89.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.9\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
90.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm8.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
99.1%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.9\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
VUB models
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-3}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
93.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
27.5%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
28.4%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm1.3\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-2}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
92.6%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm.8\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
30.8%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
50.0%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm4.8\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $10^{-1}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
89.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm2.0\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
99.2%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0.5\%$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
100%
\end_layout

\begin_layout Plain Layout
\begin_inset Formula $\pm0\%$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Evaluation for vanilla, VIB and VUB models, average over 5 runs with standard
 deviation over the IMDB dataset.
 First column is performance on the test set (higher is better 
\begin_inset Formula $\uparrow$
\end_inset

), second is % of successful Deep Word Bug attacks (lower is better 
\begin_inset Formula $\downarrow$
\end_inset

) and the third column is % of successful PWWS attacks (lower is better
 
\begin_inset Formula $\downarrow$
\end_inset

).
 
\begin_inset CommandInset label
LatexCommand label
name "tab:text_attack_evaluation"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="1">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top" width="8cm">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Text perturbed with DWB
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
g
\series bold
\bar under
n
\series default
\bar default
reat historical movie, will not allow a viewer to leave once you begin to
 watch.
 View is presented differently than displayed by most school books on this
 s
\series bold
\bar under
S
\series default
\bar default
ubject [...]
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\series bold
Text perturbed with PWWS
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
the acting , costumes , music , cinematography and sound are all 
\strikeout on
astounding
\series bold
\bar under
\strikeout default
dumbfounding
\series default
\bar default
 given the production 's austere locales .
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Examples of successful DWB and PWWS perturbations on a vanilla Bert model
 fine tuned over the IMDB dataset.
 The original input strings were perturbed such that inserted tokens are
 marked in underscored boldface and removed tokens in strikethrough.
 Both examples were classified correctly as 'Positive sentiment' before
 the attack and 'Negative sentiment' afterwards.
 
\begin_inset CommandInset label
LatexCommand label
name "tab:text_attack_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Discussion
\begin_inset CommandInset label
LatexCommand label
name "sec:Discussion"

\end_inset


\end_layout

\begin_layout Standard
While providing a complete framework for optimal data modeling, the IB,
 and its variational approximations, rely on three assumptions: (1) It
 suffices to optimize the mutual information metric to optimize a models
 performance; (2) Forgetting more information about the input while keeping
 the same information about the output induces better generalization; (3)
 Mutual information between the input, output and latent representation
 can be either computed or approximated to a desired level of accuracy.
 Our study strengthens the argument for using the Information Bottleneck
 combined with variational approximations to obtain robust models that can
 withstand adversarial attacks.
 By deriving a tighter bound on the IB functional, we demonstrate it's utility
 as the Variational Upper Bound (VUB) objective for neural networks.
 We demonstrate that VUB outperforms the Variational Information Bottleneck
 (VIB) in terms of test accuracy while providing similar or superior robustness
 to adversarial attacks in challenging classification tasks of different
 modalities, suggesting an improvement in data modeling quality.
\end_layout

\begin_layout Standard
Comparing VIB and VUB we observe that both methods promote a disentangled
 latent space by using a stochastic factorized prior, as suggested by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Chen2018}
\end_layout

\end_inset

.
 In addition, both methods utilize KL regularization, enforcing clustering
 around a 
\begin_inset Formula $0$
\end_inset

 mean which might increase latent smoothness.
 These traits can make it difficult for minor perturbations to significantly
 alter latent semantics, making the models more robust to attacks.
 In the case of VUB, the enhanced results induced by classifier regularization
 not only reinforce previous studies on the ELBO function, which suggest
 that overly powerful decoders diminish the quality of learned representations
 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
cite{Alemi2018}
\end_layout

\end_inset

, but also align with the confidence penalty proposed by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Pereyra2017}
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
In addition, we observed that in many cases VIB achieves lower validation
 set cross entropy while VUB achieves significantly higher test set accuracy.
 We attribute this gap to the VUB models becoming more calibrated, and we
 suggest that practitioners also monitor validation set accuracy and rate-distor
tion ratio during training.
 These metrics may be more informative indicators of model performance than
 validation set cross entropy alone, as validation cross entropy could increase
 as models become more calibrated.
 
\end_layout

\begin_layout Standard
We made another interesting observation during our study regarding information
 plane behavior throughout the training process.
 While previous research has documented the occurrence of error minimization
 and representation compression phases, our work revealed that these phases
 can occur in cycles throughout training.
 This finding is particularly noteworthy because previous studies observed
 this phenomenon in simple toy problems, whereas our research demonstrated
 it in complex tasks of high dimensionality with unknown distributions.
 This suggests that this information plane behavior is not limited to simplified
 scenarios but is a characteristic of the learning process in more challenging
 tasks as well.
 
\end_layout

\begin_layout Standard
In conclusion, while the IB and its variational approximations do not provide
 a complete theoretical framework for DNN data modeling and regularization,
 they offer a strong, measurable and theoretically grounded approach.
 VUB is presented as a tractable and tighter upper bound of the IB functional
 that can be easily adapted to any classifier DNN, including transformer
 based text classifiers, to significantly increase robustness to various
 adversarial attacks while inflicting minimal decrease in test set performance,
 and in some cases even increasing it.
 
\end_layout

\begin_layout Standard
This study opens many opportunities for further research.
 Besides further improvements to the upper bound, it is intriguing to use
 VUB in self-supervised learning and in generative tasks.
 Other possible directions, including measuring model calibration as proposed
 by 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
citet{Achille2018}
\end_layout

\end_inset

 are left for future work.
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/nirweingarten/Desktop/university/idc/thesis/icml/media/two_images/untargeted_two_images.png
	scale 25

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Successful untargeted FGS attack examples.
 Images are perturbations of previously successfully classified instances
 from the ImageNet validation set.
 Perturbation magnitude is determined by the parameter 
\begin_inset Formula $\epsilon$
\end_inset

 shown on the left, the higher the more perturbed.
 Original and wrongly assigned labels are listed at the top of each image.
 Notice the deterioration of image quality as 
\begin_inset Formula $\epsilon$
\end_inset

 increases.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:untargeted_examples"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage clearpage
\end_inset


\end_layout

\begin_layout Section*
Impact
\end_layout

\begin_layout Standard
This paper presents work whose goal is to advance the field of Machine Learning.
 There are many potential societal consequences of our work, none which
 we feel must be specifically highlighted here.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
bibliography{draft.bib}
\end_layout

\end_inset


\end_layout

\end_body
\end_document
